@misc{zhuMultiAgentBenchEvaluatingCollaboration2025a,
 abstract = {Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3%. Code and datasets are public available at https://github.com/MultiagentBench/MARBLE.},
 archiveprefix = {arXiv},
 author = {Zhu, Kunlun and Du, Hongyi and Hong, Zhaochen and Yang, Xiaocheng and Guo, Shuyi and Wang, Zhe and Wang, Zhenhailong and Qian, Cheng and Tang, Xiangru and Ji, Heng and You, Jiaxuan},
 doi = {10.48550/arXiv.2503.01935},
 eprint = {2503.01935},
 file = {/Users/yiyangwang/Zotero/storage/ZC7WD34G/Zhu et al. - 2025 - MultiAgentBench Evaluating the Collaboration and Competition of LLM agents.pdf;/Users/yiyangwang/Zotero/storage/6UXYUMIU/2503.html},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Multiagent Systems},
 month = {March},
 number = {arXiv:2503.01935},
 primaryclass = {cs},
 publisher = {arXiv},
 shorttitle = {MultiAgentBench},
 title = {MultiAgentBench: Evaluating the Collaboration and Competition of LLM Agents},
 urldate = {2025-06-19},
 year = {2025}
}
