---
title: 'PersonaGym: Evaluating Persona Agents and LLMs'
authors:
- Vinay Samuel
- Henry Peng Zou
- Yue Zhou
- Shreyas Chaudhari
- Ashwin Kalyan
- Tanmay Rajpurohit
- Ameet Deshpande
- Karthik Narasimhan
- Vishvak Murahari
date: '2025-05-01'
publishDate: '2025-07-12T20:29:44.318164Z'
publication_types:
- manuscript
publication: '*arXiv*'
doi: 10.48550/arXiv.2407.18416
abstract: Persona agents, which are LLM agents conditioned to act according to an
  assigned persona, enable contextually rich and user aligned interactions across
  domains like education and healthcare. However, evaluating how faithfully these
  agents adhere to their personas remains a significant challenge, particularly in
  free-form settings that demand consistency across diverse, persona-relevant environments.
  We introduce PersonaGym, the first dynamic evaluation framework for persona agents,
  and PersonaScore, a human-aligned automatic metric grounded in decision theory that
  enables comprehensive large-scale evaluation. Our evaluation of 10 leading LLMs
  across 200 personas and 10,000 questions reveals significant advancement opportunities.
  For example, GPT-4.1 had the exact same PersonaScore as LLaMA-3-8b despite being
  a more recent and advanced closed source model. Importantly, increased model size
  and complexity do not necessarily enhance persona agent capabilities, underscoring
  the need for algorithmic and architectural innovation toward faithful, performant
  persona agents.
tags:
- Computer Science - Artificial Intelligence
- Computer Science - Computation and Language
- Computer Science - Machine Learning
links:
- name: arXiv
  url: https://arxiv.org/abs/2407.18416
---
