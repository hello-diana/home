@misc{samuelPersonaGymEvaluatingPersona2025,
 abstract = {Persona agents, which are LLM agents conditioned to act according to an assigned persona, enable contextually rich and user aligned interactions across domains like education and healthcare. However, evaluating how faithfully these agents adhere to their personas remains a significant challenge, particularly in free-form settings that demand consistency across diverse, persona-relevant environments. We introduce PersonaGym, the first dynamic evaluation framework for persona agents, and PersonaScore, a human-aligned automatic metric grounded in decision theory that enables comprehensive large-scale evaluation. Our evaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals significant advancement opportunities. For example, GPT-4.1 had the exact same PersonaScore as LLaMA-3-8b despite being a more recent and advanced closed source model. Importantly, increased model size and complexity do not necessarily enhance persona agent capabilities, underscoring the need for algorithmic and architectural innovation toward faithful, performant persona agents.},
 archiveprefix = {arXiv},
 author = {Samuel, Vinay and Zou, Henry Peng and Zhou, Yue and Chaudhari, Shreyas and Kalyan, Ashwin and Rajpurohit, Tanmay and Deshpande, Ameet and Narasimhan, Karthik and Murahari, Vishvak},
 doi = {10.48550/arXiv.2407.18416},
 eprint = {2407.18416},
 file = {/Users/yiyangwang/Zotero/storage/BAH6KEP9/Samuel et al. - 2025 - PersonaGym Evaluating Persona Agents and LLMs.pdf;/Users/yiyangwang/Zotero/storage/MRF8KZ7Q/2407.html},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
 month = {May},
 number = {arXiv:2407.18416},
 primaryclass = {cs},
 publisher = {arXiv},
 shorttitle = {PersonaGym},
 title = {PersonaGym: Evaluating Persona Agents and LLMs},
 urldate = {2025-06-19},
 year = {2025}
}
