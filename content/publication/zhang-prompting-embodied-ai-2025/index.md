---
title: 'Prompting an Embodied AI Agent: How Embodiment and Multimodal Signaling Affects
  Prompting Behaviour'
authors:
- Tianyi Zhang
- Colin Au Yeung
- Emily Aurelia
- Yuki Onishi
- Neil Chulpongsatorn
- Jiannan Li
- Anthony Tang
date: '2025-04-01'
publishDate: '2025-07-12T20:29:44.464104Z'
publication_types:
- paper-conference
publication: '*Proceedings of the 2025 CHI Conference on Human Factors in Computing
  Systems*'
doi: 10.1145/3706598.3713110
abstract: Current voice agents wait for a user to complete their verbal instruction
  before responding; yet, this is misaligned with how humans engage in everyday conversational
  interaction, where interlocutors use multimodal signaling (e.g. nodding, grunting,
  or looking at referred to objects) to ensure conversational grounding. We designed
  an embodied VR agent that exhibits multimodal signaling behaviors in response to
  situated prompts, by turning its head, or by visually highlighting objects being
  discussed or referred to. We explore how people prompt this agent to design and
  manipulate the objects in a VR scene. Through a Wizard of Oz study, we found that
  participants interacting with an agent that indicated its understanding of spatial
  and action references were able to prevent errors 30% of the time, and were more
  satisfied and confident in the agent's abilities. These findings underscore the
  importance of designing multimodal signaling communication techniques for future
  embodied agents.
---
