@misc{yangSetofMarkPromptingUnleashes2023,
 abstract = {We present Set-of-Mark (SoM), a new visual prompting method, to unleash the visual grounding abilities of large multimodal models (LMMs), such as GPT-4V. As illustrated in Fig. 1 (right), we employ off-the-shelf interactive segmentation models, such as SEEM/SAM, to partition an image into regions at different levels of granularity, and overlay these regions with a set of marks e.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can answer the questions that require visual grounding. We perform a comprehensive empirical study to validate the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks. For example, our experiments show that GPT-4V with SoM in zero-shot setting outperforms the state-of-the-art fully-finetuned referring expression comprehension and segmentation model on RefCOCOg. Code for SoM prompting is made public at: https://github.com/microsoft/SoM.},
 archiveprefix = {arXiv},
 author = {Yang, Jianwei and Zhang, Hao and Li, Feng and Zou, Xueyan and Li, Chunyuan and Gao, Jianfeng},
 doi = {10.48550/arXiv.2310.11441},
 eprint = {2310.11441},
 file = {/Users/yiyangwang/Zotero/storage/HFQBHUBU/Yang et al. - 2023 - Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V.pdf;/Users/yiyangwang/Zotero/storage/H33XV57W/2310.html},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction},
 month = {November},
 number = {arXiv:2310.11441},
 primaryclass = {cs},
 publisher = {arXiv},
 title = {Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V},
 urldate = {2025-06-06},
 year = {2023}
}
