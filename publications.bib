@inproceedings{abeExaminingFactorsThat2021,
  title = {Examining the {{Factors}} That {{Make Co-Watching}} with {{Agents Effective}}},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Human-Agent Interaction}}},
  author = {Abe, Masaki and Okuoka, Kohei and Osawa, Masahiko},
  year = {2021},
  month = nov,
  pages = {354--357},
  publisher = {ACM},
  address = {Virtual Event Japan},
  doi = {10.1145/3472307.3484654},
  urldate = {2025-05-23},
  isbn = {978-1-4503-8620-3},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/UK9QT3WI/Abe et al. - 2021 - Examining the Factors that Make Co-Watching with Agents Effective.pdf}
}

@article{alshurafaRationaleDesignSenseWhy2023,
  title = {Rationale and Design of the {{SenseWhy}} Project: {{A}} Passive Sensing and Ecological Momentary Assessment Study on Characteristics of Overeating Episodes},
  shorttitle = {Rationale and Design of the {{SenseWhy}} Project},
  author = {Alshurafa, Nabil I. and Stump, Tammy K. and Romano, Christopher S. and F. Pfammatter, Angela and Lin, Annie W. and Hester, Josiah and Hedeker, Donald and Forman, Evan and Spring, Bonnie},
  year = {2023},
  month = jan,
  journal = {DIGITAL HEALTH},
  volume = {9},
  pages = {20552076231158314},
  publisher = {SAGE Publications Ltd},
  issn = {2055-2076},
  doi = {10.1177/20552076231158314},
  urldate = {2024-11-12},
  abstract = {ObjectivesOvereating interventions and research often focus on single determinants and use subjective or nonpersonalized measures. We aim to (1) identify automatically detectable features that predict overeating and (2) build clusters of eating episodes that identify theoretically meaningful and clinically known problematic overeating behaviors (e.g., stress eating), as well as new phenotypes based on social and psychological features.MethodUp to 60 adults with obesity in the Chicagoland area will be recruited for a 14-day free-living observational study. Participants will complete ecological momentary assessments and wear 3 sensors designed to capture features of overeating episodes (e.g., chews) that can be visually confirmed. Participants will also complete daily dietitian-administered 24-hour recalls of all food and beverages consumed.AnalysisOvereating is defined as caloric consumption exceeding 1 standard deviation of an individual's mean consumption per eating episode. To identify features that predict overeating, we will apply 2 complementary machine learning methods: correlation-based feature selection and wrapper-based feature selection. We will then generate clusters of overeating types and assess how they align with clinically meaningful overeating phenotypes.ConclusionsThis study will be the first to assess characteristics of eating episodes in situ over a multiweek period with visual confirmation of eating behaviors. An additional strength of this study is the assessment of predictors of problematic eating during periods when individuals are not on a structured diet and/or engaged in a weight loss intervention. Our assessment of overeating episodes in real-world settings is likely to yield new insights regarding determinants of overeating that may translate into novel interventions.},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/TV5265UA/Alshurafa et al. - 2023 - Rationale and design of the SenseWhy project A pa.pdf}
}

@inproceedings{andrewsAiCommentatorMultimodalConversational2024,
  title = {{{AiCommentator}}: {{A Multimodal Conversational Agent}} for {{Embedded Visualization}} in {{Football Viewing}}},
  shorttitle = {{{AiCommentator}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Andrews, Peter and Nordberg, Oda Elise and Zubicueta Portales, Stephanie and Borch, Nj{\aa}l and Guribye, Frode and Fujita, Kazuyuki and Fjeld, Morten},
  year = {2024},
  month = mar,
  pages = {14--34},
  publisher = {ACM},
  address = {Greenville SC USA},
  doi = {10.1145/3640543.3645197},
  urldate = {2025-05-27},
  isbn = {979-8-4007-0508-3},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/QJW7HIBK/Andrews et al. - 2024 - AiCommentator A Multimodal Conversational Agent for Embedded Visualization in Football Viewing.pdf}
}

@inproceedings{barryMHealthMaternalMental2017,
  title = {{{mHealth}} for {{Maternal Mental Health}}: {{Everyday Wisdom}} in {{Ethical Design}}},
  shorttitle = {{{mHealth}} for {{Maternal Mental Health}}},
  booktitle = {Proceedings of the 2017 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Barry, Marguerite and Doherty, Kevin and Marcano Belisario, Jose and Car, Josip and Morrison, Cecily and Doherty, Gavin},
  year = {2017},
  month = may,
  pages = {2708--2756},
  publisher = {ACM},
  address = {Denver Colorado USA},
  doi = {10.1145/3025453.3025918},
  urldate = {2025-02-12},
  abstract = {Health and wellbeing applications increasingly raise ethical issues for design. User-centred and participatory design approaches, while grounded in everyday wisdom, cannot be expected to address ethical reflection consistently, as multiple value systems come into play. We explore the potential of phronesis, a concept from Aristotelian virtue ethics, for mHealth design. Phronesis describes wisdom and judgment garnered from practical experience of specific situations in context. Applied phronesis contributes everyday wisdom to challenging issues for vulnerable target users. Drawing on research into mHealth technologies for psychological wellbeing, we explore how phronesis can inform ethical design. Using a case study on an app for selfreporting symptoms of depression during pregnancy, we present a framework for incorporating a phronetic approach into design, involving: (a) a wide feedback net to capture phronetic input early in design; (b) observing the order of feedback, which directly affects value priorities in design; (c) ethical pluralism recognising different coexisting value systems; (d) acknowledging subjectivity in the disclosure and recognition of individual researcher and participant values. We offer insights into how a phronetic approach can contribute everyday wisdom to designing mHealth technologies to help designers foster the values that promote human flourishing.},
  isbn = {978-1-4503-4655-9},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/6NXDAXDS/Barry et al. - 2017 - mHealth for Maternal Mental Health Everyday Wisdo.pdf}
}

@article{brookeSUSQuickDirty,
  title = {{{SUS}} - {{A}} Quick and Dirty Usability Scale},
  author = {Brooke, John},
  abstract = {Usability does not exist in any absolute sense; it can only be defined with reference to particular contexts. This, in turn, means that there are no absolute measures of usability, since, if the usability of an artefact is defined by the context in which that artefact is used, measures of usability must of necessity be defined by that context too. Despite this, there is a need for broad general measures which can be used to compare usability across a range of contexts. In addition, there is a need for ``quick and dirty'' methods to allow low cost assessments of usability in industrial systems evaluation. This chapter describes the System Usability Scale (SUS) a reliable, low-cost usability scale that can be used for global assessments of systems usability.},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/3RMMKXV3/Brooke - SUS - A quick and dirty usability scale.pdf}
}

@book{burrellSociologicalParadigmsOrganisational2011,
  title = {Sociological Paradigms and Organisational Analysis: Elements of the Sociology of Corporate Life},
  shorttitle = {Sociological Paradigms and Organisational Analysis},
  author = {Burrell, Gibson and Morgan, Gareth},
  year = {2011},
  edition = {Reprinted},
  publisher = {Ashgate},
  address = {Farnham},
  isbn = {978-0-566-05148-7 978-1-85742-114-9},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/9L6XZUIP/Burrell and Morgan - 2011 - Sociological paradigms and organisational analysis.pdf}
}

@article{cabralReview4113Communication,
  title = {Review \#4113 "{{Communication Matters}}: {{Technology}} for {{Information}} ..."},
  author = {Cabral, Alex},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/EMDB7M3W/Cabral - Review #4113 Communication Matters Technology fo.pdf}
}

@inproceedings{campowoytukTactfulFeministSensing2023,
  title = {Tactful {{Feminist Sensing}}: {{Designing}} for {{Touching Vaginal Fluids}}},
  shorttitle = {Tactful {{Feminist Sensing}}},
  booktitle = {Proceedings of the 2023 {{ACM Designing Interactive Systems Conference}}},
  author = {Campo Woytuk, Nadia and Park, Joo Young and Maslik, Jan and Ciolfi Felice, Marianela and Balaam, Madeline},
  year = {2023},
  month = jul,
  pages = {2642--2656},
  publisher = {ACM},
  address = {Pittsburgh PA USA},
  doi = {10.1145/3563657.3595966},
  urldate = {2025-02-05},
  isbn = {978-1-4503-9893-0},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/DHHDDGHX/Campo Woytuk et al. - 2023 - Tactful Feminist Sensing Designing for Touching V.pdf}
}

@inproceedings{chenEmotionawareDesignAutomobiles2025,
  title = {Emotion-Aware {{Design}} in {{Automobiles}}: {{Embracing Technology Advancements}} to {{Enhance Human-vehicle Interaction}}},
  shorttitle = {Emotion-Aware {{Design}} in {{Automobiles}}},
  booktitle = {Proceedings of the 2025 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Chen, Xingtong and Wang, Xia and Fang, Cong and Fang, Le and Gong, Wei and Liu, Chengzhong and Wang, Stephen Jia},
  year = {2025},
  month = apr,
  pages = {1--18},
  publisher = {ACM},
  address = {Yokohama Japan},
  doi = {10.1145/3706598.3713571},
  urldate = {2025-05-11},
  isbn = {979-8-4007-1394-1},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/4SQ93X8W/Chen et al. - 2025 - Emotion-aware Design in Automobiles Embracing Technology Advancements to Enhance Human-vehicle Inte.pdf}
}

@inproceedings{choiASpireClippableMobile2021,
  title = {{{aSpire}}: {{Clippable}}, {{Mobile Pneumatic-Haptic Device}} for {{Breathing Rate Regulation}} via {{Personalizable Tactile Feedback}}},
  shorttitle = {{{aSpire}}},
  booktitle = {Extended {{Abstracts}} of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Choi, Kyung Yun and Lee, Jinmo and ElHaouij, Neska and Picard, Rosalind and Ishii, Hiroshi},
  year = {2021},
  month = may,
  pages = {1--8},
  publisher = {ACM},
  address = {Yokohama Japan},
  doi = {10.1145/3411763.3451602},
  urldate = {2025-05-13},
  abstract = {We introduce--aSpire--a clippable, mobile pneumatic-haptic device designed to help users regulate their breathing rate via subtle tactile feedback. aSpire can be easily clipped to a strap/belt and used to personalize tactile stimulation patterns, intensity, and frequency via its array of air pouch actuators that infate/defate individually. To evaluate the efectiveness of aSpire's diferent tactile stimulation patterns in guiding the breathing rate of people on the move, outof-lab environment, we conducted a user study with car passengers in a real-world commuting setting. The results show that engaging with the aSpire does not evoke extra mental stress, and helps the participants reduce their average breathing rate while keeping their perceived pleasantness and energy level high.},
  isbn = {978-1-4503-8095-9},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/8Q5J75CC/Choi et al. - 2021 - aSpire Clippable, Mobile Pneumatic-Haptic Device for Breathing Rate Regulation via Personalizable T.pdf}
}

@article{CommunicationMattersTechnology,
  title = {Communication {{Matters}}: {{Technology}} for {{Information Exchange Needs}} in the {{Context}} of {{Prostate Cancer Diagnosis}}},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/6IRJ7QGP/Communication Matters Technology for Information .pdf}
}

@misc{crawfordBMWAgentsFramework2024,
  title = {{{BMW Agents}} -- {{A Framework For Task Automation Through Multi-Agent Collaboration}}},
  author = {Crawford, Noel and Duffy, Edward B. and Evazzade, Iman and Foehr, Torsten and Robbins, Gregory and Saha, Debbrata Kumar and Varma, Jiya and Ziolkowski, Marcin},
  year = {2024},
  month = jul,
  number = {arXiv:2406.20041},
  eprint = {2406.20041},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.20041},
  urldate = {2025-06-03},
  abstract = {Autonomous agents driven by Large Language Models (LLMs) offer enormous potential for automation. Early proof of this technology can be found in various demonstrations of agents solving complex tasks, interacting with external systems to augment their knowledge, and triggering actions. In particular, workflows involving multiple agents solving complex tasks in a collaborative fashion exemplify their capacity to operate in less strict and less well-defined environments. Thus, a multi-agent approach has great potential for serving as a backbone in many industrial applications, ranging from complex knowledge retrieval systems to next generation robotic process automation. Given the reasoning abilities within the current generation of LLMs, complex processes require a multi-step approach that includes a plan of well-defined and modular tasks. Depending on the level of complexity, these tasks can be executed either by a single agent or a group of agents. In this work, we focus on designing a flexible agent engineering framework with careful attention to planning and execution, capable of handling complex use case applications across various domains. The proposed framework provides reliability in industrial applications and presents techniques to ensure a scalable, flexible, and collaborative workflow for multiple autonomous agents working together towards solving tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {/Users/yiyangwang/Zotero/storage/ZVSL84L8/Crawford et al. - 2024 - BMW Agents -- A Framework For Task Automation Through Multi-Agent Collaboration.pdf;/Users/yiyangwang/Zotero/storage/HTJUQ3J7/2406.html}
}

@inproceedings{cruzEquityWareCoDesigningWearables2023,
  title = {{{EquityWare}}: {{Co-Designing Wearables With And For Low Income Communities In The U}}.{{S}}.},
  shorttitle = {{{EquityWare}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Cruz, Stefany and Redding, Alexander and Chau, Connie W. and Lu, Claire and Persche, Julia and Hester, Josiah and Jacobs, Maia},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--18},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3544548.3580980},
  urldate = {2024-11-12},
  abstract = {Wearables are a potentially vital mechanism for individuals to monitor their health, track behaviors, and stay connected. Unfortunately, both price and a lack of consideration of the needs of low-SES communities have made these devices inaccessible and unusable for communities that would most substantially benefit from their affordances. To address this gap and better understand how members of low-SES communities perceive the potential benefits and barriers to using wearable devices, we conducted 19 semi-structured interviews with people from minority, high crime rate, low-SES communities. Participants emphasized a critical need for safety-related wearable devices in their communities. Still, existing tools do not yet address the specific needs of this community and are out of reach due to several barriers. We distill themes on perceived useful features and ongoing obstacles to guide a much-needed research agenda we term 'Equityware': building wearable devices based on low-SES communities' needs, comfortability, and limitations.},
  isbn = {978-1-4503-9421-5},
  file = {/Users/yiyangwang/Zotero/storage/88WY6IMW/Cruz et al. - 2023 - EquityWare Co-Designing Wearables With And For Lo.pdf}
}

@inproceedings{cruzEquityWareCoDesigningWearables2023a,
  title = {{{EquityWare}}: {{Co-Designing Wearables With And For Low Income Communities In The U}}.{{S}}.},
  shorttitle = {{{EquityWare}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Cruz, Stefany and Redding, Alexander and Chau, Connie W. and Lu, Claire and Persche, Julia and Hester, Josiah and Jacobs, Maia},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--18},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3544548.3580980},
  urldate = {2024-11-12},
  abstract = {Wearables are a potentially vital mechanism for individuals to monitor their health, track behaviors, and stay connected. Unfortunately, both price and a lack of consideration of the needs of low-SES communities have made these devices inaccessible and unusable for communities that would most substantially benefit from their affordances. To address this gap and better understand how members of low-SES communities perceive the potential benefits and barriers to using wearable devices, we conducted 19 semi-structured interviews with people from minority, high crime rate, low-SES communities. Participants emphasized a critical need for safety-related wearable devices in their communities. Still, existing tools do not yet address the specific needs of this community and are out of reach due to several barriers. We distill themes on perceived useful features and ongoing obstacles to guide a much-needed research agenda we term 'Equityware': building wearable devices based on low-SES communities' needs, comfortability, and limitations.},
  isbn = {978-1-4503-9421-5},
  file = {/Users/yiyangwang/Zotero/storage/9JCXV3VN/Cruz et al. - 2023 - EquityWare Co-Designing Wearables With And For Lo.pdf}
}

@article{cuadraDigitalFormsAll2024,
  title = {Digital {{Forms}} for {{All}}: {{A Holistic Multimodal Large Language Model Agent}} for {{Health Data Entry}}},
  shorttitle = {Digital {{Forms}} for {{All}}},
  author = {Cuadra, Andrea and Breuch, Justine and Estrada, Samantha and Ihim, David and Hung, Isabelle and Askaryar, Derek and Hassanien, Marwan and Fessele, Kristen L. and Landay, James A.},
  year = {2024},
  month = may,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {8},
  number = {2},
  pages = {1--39},
  issn = {2474-9567},
  doi = {10.1145/3659624},
  urldate = {2025-05-14},
  abstract = {Digital forms help us access services and opportunities, but they are not equally accessible to everyone, such as older adults or those with sensory impairments. Large language models (LLMs) and multimodal interfaces offer a unique opportunity to increase form accessibility. Informed by prior literature and needfinding, we built a holistic multimodal LLM agent for health data entry. We describe the process of designing and building our system, and the results of a study with older adults (N =10). All participants, regardless of age or disability status, were able to complete a standard 47-question form independently using our system---one blind participant said it was "a prayer answered." Our video analysis revealed how different modalities provided alternative interaction paths in complementary ways (e.g., the buttons helped resolve transcription errors and speech helped provide more options when the pre-canned answer choices were insufficient). We highlight key design guidelines, such as designing systems that dynamically adapt to individual needs.},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/N7BRA228/Cuadra et al. - 2024 - Digital Forms for All A Holistic Multimodal Large Language Model Agent for Health Data Entry.pdf}
}

@article{cuschieriCONSORTStatement2019,
  title = {The {{CONSORT}} Statement},
  author = {Cuschieri, Sarah},
  year = {2019},
  month = apr,
  journal = {Saudi Journal of Anaesthesia},
  volume = {13},
  number = {Suppl 1},
  pages = {S27-S30},
  issn = {1658-354X},
  doi = {10.4103/sja.SJA_559_18},
  urldate = {2025-02-09},
  abstract = {Randomized control trials (RCT's) are the gold standard in evaluating and efficiently translating research data into clinical practice. The CONSORT statement was conceptualized in order to help ascertain standardization and reproducibility of RCT's. The articles publishing the CONSORT 2010 statement along with their bibliographies were identified and thoroughly reviewed. The CONSORT statement is made up of a 25-item checklist that provides the author with a solid backbone around which to construct and present an RCT. It sets standards on the trial's design, analysis, and interpretation of the results.},
  pmcid = {PMC6398298},
  pmid = {30930716},
  file = {/Users/yiyangwang/Zotero/storage/RTWPBYZW/Cuschieri - 2019 - The CONSORT statement.pdf}
}

@misc{DesignKit,
  title = {Design {{Kit}}},
  urldate = {2025-02-12},
  howpublished = {https://www.designkit.org/methods.html},
  file = {/Users/yiyangwang/Zotero/storage/LPVIAFWQ/methods.html}
}

@misc{DesignKita,
  title = {Design {{Kit}}},
  urldate = {2025-02-12},
  howpublished = {https://www.designkit.org/methods.html},
  file = {/Users/yiyangwang/Zotero/storage/3PH9UBHE/methods.html}
}

@misc{DetectingEatingSocial,
  title = {Detecting {{Eating}}, and {{Social Presence}} with {{All Day Wearable RGB-T}} {\textbar} {{Proceedings}} of the 8th {{ACM}}/{{IEEE International Conference}} on {{Connected Health}}: {{Applications}}, {{Systems}} and {{Engineering Technologies}}},
  urldate = {2024-11-12},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/3580252.3586974}
}

@inproceedings{earleDreamGardenDesignerAssistant2025,
  title = {{{DreamGarden}}: {{A Designer Assistant}} for {{Growing Games}} from a {{Single Prompt}}},
  shorttitle = {{{DreamGarden}}},
  booktitle = {Proceedings of the 2025 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Earle, Sam and Parajuli, Samyak and {Banburski-Fahey}, Andrzej},
  year = {2025},
  month = apr,
  pages = {1--19},
  publisher = {ACM},
  address = {Yokohama Japan},
  doi = {10.1145/3706598.3714233},
  urldate = {2025-07-07},
  copyright = {https://www.acm.org/publications/policies/copyright\_policy\#Background},
  langid = {american},
  file = {/Users/yiyangwang/Zotero/storage/V5TR5I4E/Earle et al. - 2025 - DreamGarden A Designer Assistant for Growing Games from a Single Prompt.pdf}
}

@misc{ehteshamSurveyAgentInteroperability2025,
  title = {A Survey of Agent Interoperability Protocols: {{Model Context Protocol}} ({{MCP}}), {{Agent Communication Protocol}} ({{ACP}}), {{Agent-to-Agent Protocol}} ({{A2A}}), and {{Agent Network Protocol}} ({{ANP}})},
  shorttitle = {A Survey of Agent Interoperability Protocols},
  author = {Ehtesham, Abul and Singh, Aditi and Gupta, Gaurav Kumar and Kumar, Saket},
  year = {2025},
  month = may,
  number = {arXiv:2505.02279},
  eprint = {2505.02279},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.02279},
  urldate = {2025-06-03},
  abstract = {Large language model powered autonomous agents demand robust, standardized protocols to integrate tools, share contextual data, and coordinate tasks across heterogeneous systems. Ad-hoc integrations are difficult to scale, secure, and generalize across domains. This survey examines four emerging agent communication protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP), each addressing interoperability in deployment contexts. MCP provides a JSON-RPC client-server interface for secure tool invocation and typed data exchange. ACP defines a general-purpose communication protocol over RESTful HTTP, supporting MIME-typed multipart messages and synchronous and asynchronous interactions. Its lightweight and runtime-independent design enables scalable agent invocation, while features like session management, message routing, and integration with role-based and decentralized identifiers (DIDs). A2A enables peer-to-peer task delegation using capability-based Agent Cards, supporting secure and scalable collaboration across enterprise agent workflows. ANP supports open network agent discovery and secure collaboration using W3C decentralized identifiers DIDs and JSON-LD graphs. The protocols are compared across multiple dimensions, including interaction modes, discovery mechanisms, communication patterns, and security models. Based on the comparative analysis, a phased adoption roadmap is proposed: beginning with MCP for tool access, followed by ACP for structured, multimodal messaging session-aware interaction and both online and offline agent discovery across scalable, HTTP-based deployments A2A for collaborative task execution, and extending to ANP for decentralized agent marketplaces. This work provides a comprehensive foundation for designing secure, interoperable, and scalable ecosystems of LLM-powered agents.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/yiyangwang/Zotero/storage/5S2I58LT/Ehtesham et al. - 2025 - A survey of agent interoperability protocols Model Context Protocol (MCP), Agent Communication Prot.pdf;/Users/yiyangwang/Zotero/storage/BJXCHEVM/2505.html}
}

@misc{EstimationChangesIntracardiac,
  title = {Estimation of {{Changes}} in {{Intracardiac Hemodynamics Using Wearable Seismocardiography}} and {{Machine Learning}} in {{Patients With Heart Failure}}: {{A Feasibility Study}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2024-11-12},
  howpublished = {https://ieeexplore.ieee.org/document/9697365}
}

@inproceedings{fangLeveragingAIGeneratedEmotional2025,
  title = {Leveraging {{AI-Generated Emotional Self-Voice}} to {{Nudge People}} towards Their {{Ideal Selves}}},
  booktitle = {Proceedings of the 2025 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Fang, Cathy Mengying and Chua, Phoebe and Chan, Samantha W. T. and Leong, Joanne and Bao, Andria and Maes, Pattie},
  year = {2025},
  month = apr,
  pages = {1--20},
  publisher = {ACM},
  address = {Yokohama Japan},
  doi = {10.1145/3706598.3713359},
  urldate = {2025-07-08},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {american},
  file = {/Users/yiyangwang/Zotero/storage/UMQX4MBJ/Fang et al. - 2025 - Leveraging AI-Generated Emotional Self-Voice to Nudge People towards their Ideal Selves.pdf}
}

@article{fernandesHabitSensePrivacyAwareAIEnhanced2024,
  title = {{{HabitSense}}: {{A Privacy-Aware}}, {{AI-Enhanced Multimodal Wearable Platform}} for {{mHealth Applications}}},
  shorttitle = {{{HabitSense}}},
  author = {Fernandes, Glenn J. and Zheng, Jiayi and Pedram, Mahdi and Romano, Christopher and Shahabi, Farzad and Rothrock, Blaine and Cohen, Thomas and Zhu, Helen and Butani, Tanmeet S. and Hester, Josiah and Katsaggelos, Aggelos K. and Alshurafa, Nabil},
  year = {2024},
  month = sep,
  journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  volume = {8},
  number = {3},
  pages = {101:1--101:48},
  doi = {10.1145/3678591},
  urldate = {2024-11-12},
  abstract = {Wearable cameras provide an objective method to visually confirm and automate the detection of health-risk behaviors such as smoking and overeating, which is critical for developing and testing adaptive treatment interventions. Despite the potential of wearable camera systems, adoption is hindered by inadequate clinician input in the design, user privacy concerns, and user burden. To address these barriers, we introduced HabitSense, an open-source1, multi-modal neck-worn platform developed with input from focus groups with clinicians (N=36) and user feedback from in-wild studies involving 105 participants over 35 days. Optimized for monitoring health-risk behaviors, the platform utilizes RGB, thermal, and inertial measurement unit sensors to detect eating and smoking events in real time. In a 7-day study involving 15 participants, HabitSense recorded 768 hours of footage, capturing 420.91 minutes of hand-to-mouth gestures associated with eating and smoking data crucial for training machine learning models, achieving a 92\% F1-score in gesture recognition. To address privacy concerns, the platform records only during likely health-risk behavior events using SECURE, a smart activation algorithm. Additionally, HabitSense employs on-device obfuscation algorithms that selectively obfuscate the background during recording, maintaining individual privacy while leaving gestures related to health-risk behaviors unobfuscated. Our implementation of SECURE has resulted in a 48\% reduction in storage needs and a 30\% increase in battery life. This paper highlights the critical roles of clinician feedback, extensive field testing, and privacy-enhancing algorithms in developing an unobtrusive, lightweight, and reproducible wearable system that is both feasible and acceptable for monitoring health-risk behaviors in real-world settings.}
}

@misc{FunctionalityUserReview,
  title = {Functionality and {{User Review Analysis}} of {{Mobile Apps}} for {{Mindfulness Eating}} and {{Eating Disorders}} {\textbar} {{Proceedings}} of the 2024 {{ACM Designing Interactive Systems Conference}}},
  urldate = {2024-11-18},
  howpublished = {https://dl.acm.org/doi/10.1145/3643834.3661521},
  file = {/Users/yiyangwang/Zotero/storage/KUPMRBLW/3643834.html}
}

@article{gantiEnablingWearablePulse2021,
  title = {Enabling {{Wearable Pulse Transit Time-Based Blood Pressure Estimation}} for {{Medically Underserved Areas}} and {{Health Equity}}: {{Comprehensive Evaluation Study}}},
  shorttitle = {Enabling {{Wearable Pulse Transit Time-Based Blood Pressure Estimation}} for {{Medically Underserved Areas}} and {{Health Equity}}},
  author = {Ganti, Venu and Carek, Andrew M. and Jung, Hewon and Srivatsa, Adith V. and Cherry, Deborah and Johnson, Levather Neicey and Inan, Omer T.},
  year = {2021},
  month = aug,
  journal = {JMIR mHealth and uHealth},
  volume = {9},
  number = {8},
  pages = {e27466},
  issn = {2291-5222},
  doi = {10.2196/27466},
  abstract = {BACKGROUND: Noninvasive and cuffless approaches to monitor blood pressure (BP), in light of their convenience and accuracy, have paved the way toward remote screening and management of hypertension. However, existing noninvasive methodologies, which operate on mechanical, electrical, and optical sensing modalities, have not been thoroughly evaluated in demographically and racially diverse populations. Thus, the potential accuracy of these technologies in populations where they could have the greatest impact has not been sufficiently addressed. This presents challenges in clinical translation due to concerns about perpetuating existing health disparities. OBJECTIVE: In this paper, we aim to present findings on the feasibility of a cuffless, wrist-worn, pulse transit time (PTT)-based device for monitoring BP in a diverse population. METHODS: We recruited a diverse population through a collaborative effort with a nonprofit organization working with medically underserved areas in Georgia. We used our custom, multimodal, wrist-worn device to measure the PTT through seismocardiography, as the proximal timing reference, and photoplethysmography, as the distal timing reference. In addition, we created a novel data-driven beat-selection algorithm to reduce noise and improve the robustness of the method. We compared the wearable PTT measurements with those from a finger-cuff continuous BP device over the course of several perturbations used to modulate BP. RESULTS: Our PTT-based wrist-worn device accurately monitored diastolic blood pressure (DBP) and mean arterial pressure (MAP) in a diverse population (N=44 participants) with a mean absolute difference of 2.90 mm Hg and 3.39 mm Hg for DBP and MAP, respectively, after calibration. Meanwhile, the mean absolute difference of our systolic BP estimation was 5.36 mm Hg, a grade B classification based on the Institute for Electronics and Electrical Engineers standard. We have further demonstrated the ability of our device to capture the commonly observed demographic differences in underlying arterial stiffness. CONCLUSIONS: Accurate DBP and MAP estimation, along with grade B systolic BP estimation, using a convenient wearable device can empower users and facilitate remote BP monitoring in medically underserved areas, thus providing widespread hypertension screening and management for health equity.},
  langid = {english},
  pmcid = {PMC8369375},
  pmid = {34338646},
  keywords = {Blood Pressure,cuffless blood pressure,health equity,Health Equity,Humans,Medically Underserved Area,mobile phone,noninvasive blood pressure estimation,pulse transit time,Pulse Wave Analysis,Wearable Electronic Devices,wearable sensing},
  file = {/Users/yiyangwang/Zotero/storage/EYZJ5UUH/Ganti et al. - 2021 - Enabling Wearable Pulse Transit Time-Based Blood P.pdf}
}

@misc{gautamSoccerNetEchoesSoccerGame2024,
  title = {{{SoccerNet-Echoes}}: {{A Soccer Game Audio Commentary Dataset}}},
  shorttitle = {{{SoccerNet-Echoes}}},
  author = {Gautam, Sushant and Sarkhoosh, Mehdi Houshmand and Held, Jan and Midoglu, Cise and Cioppa, Anthony and Giancola, Silvio and Thambawita, Vajira and Riegler, Michael A. and Halvorsen, P{\aa}l and Shah, Mubarak},
  year = {2024},
  month = may,
  number = {arXiv:2405.07354},
  eprint = {2405.07354},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.07354},
  urldate = {2025-06-09},
  abstract = {The application of Automatic Speech Recognition (ASR) technology in soccer offers numerous opportunities for sports analytics. Specifically, extracting audio commentaries with ASR provides valuable insights into the events of the game, and opens the door to several downstream applications such as automatic highlight generation. This paper presents SoccerNet-Echoes, an augmentation of the SoccerNet dataset with automatically generated transcriptions of audio commentaries from soccer game broadcasts, enhancing video content with rich layers of textual information derived from the game audio using ASR. These textual commentaries, generated using the Whisper model and translated with Google Translate, extend the usefulness of the SoccerNet dataset in diverse applications such as enhanced action spotting, automatic caption generation, and game summarization. By incorporating textual data alongside visual and auditory content, SoccerNet-Echoes aims to serve as a comprehensive resource for the development of algorithms specialized in capturing the dynamics of soccer games. We detail the methods involved in the curation of this dataset and the integration of ASR. We also highlight the implications of a multimodal approach in sports analytics, and how the enriched dataset can support diverse applications, thus broadening the scope of research and development in the field of sports analytics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/yiyangwang/Zotero/storage/3IMP4JSC/Gautam et al. - 2024 - SoccerNet-Echoes A Soccer Game Audio Commentary Dataset.pdf;/Users/yiyangwang/Zotero/storage/L6QX2RXR/2405.html}
}

@article{goelPhantomPuffsPhantom,
  title = {Phantom {{Puffs}}: {{A Phantom Lung}} to {{Emulate Smoking Behavior}}},
  author = {Goel, Rishabh and Wang, Yiyang and Adams, Alexander T},
  journal = {Online Journal of Robotics \& Automation Technology},
  volume = {3},
  number = {2},
  publisher = {Iris Publishers},
  abstract = {Testing sensors on human subjects is fraught with challenges such as extensive setup times and inconsistent data collection. This study explores the use of phantom organs as a solution to streamline sensor testing and validation. By replicating essential organ functions, phantom organs can provide a consistent and repeatable testing environment, improving data accuracy and reliability. Our development of a phantom lung, capable of emulating human breathing patterns, exemplifies this approach. This phantom lung could facilitate the testing of ENDS monitoring sensors, allowing for the safe and controlled simulation of smoking combustible and electronic cigarettes. The use of phantom organs not only reduces reliance on human subjects in sensor development but also fosters innovation by allowing experimentation with novel sensing mechanisms. This research highlights the potential of phantom organs to revolutionize sensor testing practices, ultimately accelerating the advancement of sensing technologies in various fields.},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/T977HV7Y/Goel et al. - Phantom Puffs A Phantom Lung to Emulate Smoking B.pdf}
}

@article{goelPhantomPuffsPhantoma,
  title = {Phantom {{Puffs}}: {{A Phantom Lung}} to {{Emulate Smoking Behavior}}},
  author = {Goel, Rishabh and Wang, Yiyang and Adams, Alexander T},
  journal = {Online Journal of Robotics \& Automation Technology},
  volume = {3},
  number = {2},
  abstract = {Testing sensors on human subjects is fraught with challenges such as extensive setup times and inconsistent data collection. This study explores the use of phantom organs as a solution to streamline sensor testing and validation. By replicating essential organ functions, phantom organs can provide a consistent and repeatable testing environment, improving data accuracy and reliability. Our development of a phantom lung, capable of emulating human breathing patterns, exemplifies this approach. This phantom lung could facilitate the testing of ENDS monitoring sensors, allowing for the safe and controlled simulation of smoking combustible and electronic cigarettes. The use of phantom organs not only reduces reliance on human subjects in sensor development but also fosters innovation by allowing experimentation with novel sensing mechanisms. This research highlights the potential of phantom organs to revolutionize sensor testing practices, ultimately accelerating the advancement of sensing technologies in various fields.},
  file = {/Users/yiyangwang/Zotero/storage/975CCNTV/Goel et al. - Phantom Puffs A Phantom Lung to Emulate Smoking B.pdf}
}

@inproceedings{guluzadeFunctionalityUserReview2024,
  title = {Functionality and {{User Review Analysis}} of {{Mobile Apps}} for {{Mindfulness Eating}} and {{Eating Disorders}}},
  booktitle = {Proceedings of the 2024 {{ACM Designing Interactive Systems Conference}}},
  author = {Guluzade, Lala and Sas, Corina},
  year = {2024},
  month = jul,
  series = {{{DIS}} '24},
  pages = {1350--1371},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3643834.3661521},
  urldate = {2024-11-18},
  abstract = {A growing number of mobile apps have focused on healthy or problematic eating, albeit limited research has focused on evaluating such apps from users' perspectives. To address this, we evaluated the functionalities of 27 apps on mindfulness eating, and eating disorders from the Apple App, and Google Play Stores, and conducted a content analysis of 1248 user reviews, totaling over 60,000 words. Findings indicate the main functionalities of tracking data on eating behaviors, emotions, thoughts, bodily sensations, symptoms, as well as triggers of eating disorders, and of providing interventions such as mindfulness, goal setting, psychoeducation, CBT, and holistic ones. Findings also highlight key usability and ethical challenges, which we used to inform five design implications namely tracking and reflecting on multiple aspects of mindfulness and healthy eating, supporting personalized interventions and AI-based ones, as well as the sensitive design for diagnosis, and for tracking and monitoring problematic data.},
  isbn = {979-8-4007-0583-0},
  file = {/Users/yiyangwang/Zotero/storage/336NNIRR/Guluzade and Sas - 2024 - Functionality and User Review Analysis of Mobile A.pdf}
}

@article{guoLX2343AlleviatesCognitive2017,
  title = {{{LX2343}} Alleviates Cognitive Impairments in {{AD}} Model Rats by Inhibiting Oxidative Stress-Induced Neuronal Apoptosis and Tauopathy},
  author = {Guo, Xiao-dan and Sun, Guang-long and Zhou, Ting-ting and Wang, Yi-yang and Xu, Xin and Shi, Xiao-fan and Zhu, Zhi-yuan and Rukachaisirikul, Vatcharin and Hu, Li-hong and Shen, Xu},
  year = {2017},
  month = aug,
  journal = {Acta Pharmacologica Sinica},
  volume = {38},
  number = {8},
  pages = {1104--1119},
  publisher = {Nature Publishing Group},
  issn = {1745-7254},
  doi = {10.1038/aps.2016.128},
  urldate = {2024-12-30},
  abstract = {Alzheimer's disease (AD) is a progressive neurodegenerative disease leading to the irreversible loss of brain neurons and cognitive abilities, and the vicious interplay between oxidative stress (OS) and tauopathy is believed to be one of the major players in AD development. Here, we demonstrated the capability of the small molecule N-(1,3-benzodioxol-5-yl)-2-[5-chloro-2-methoxy(phenylsulfonyl)anilino]acetamide (LX2343) to ameliorate the cognitive dysfunction of AD model rats by inhibiting OS-induced neuronal apoptosis and tauopathy. Streptozotocin (STZ) was used to induce OS in neuronal cells in vitro and in AD model rats that were made by intracerebroventricular injection of STZ (3 mg/kg, bilaterally), and Morris water maze test was used to evaluate the cognitive dysfunction in ICV-STZ rats. Treatment with LX2343 (5--20 {$\mu$}mol/L) significantly attenuated STZ-induced apoptosis in SH-SY5Y cells and mouse primary cortical neurons by alleviating OS and inhibiting the JNK/p38 and pro-apoptotic pathways. LX2343 was able to restore the integrity of mitochondrial function and morphology, increase ATP biosynthesis, and reduce ROS accumulation in the neuronal cells. In addition, LX2343 was found to be a non-ATP competitive GSK-3{$\beta$} inhibitor with IC50 of 1.84{\textpm}0.07 {$\mu$}mol/L, and it potently inhibited tau hyperphosphorylation in the neuronal cells. In ICV-STZ rats, administration of LX2343 (7, 21 mg{$\cdot$}kg-1{$\cdot$}d-1, ip, for 5 weeks) efficiently improved their cognitive deficits. LX2343 ameliorates the cognitive dysfunction in the AD model rats by suppressing OS-induced neuronal apoptosis and tauopathy, thus highlighting the potential of LX2343 for the treatment of AD.},
  copyright = {2017 CPS and SIMM},
  langid = {english},
  keywords = {Biomedicine,general,Immunology,Internal Medicine,Medical Microbiology,Pharmacology/Toxicology,Vaccine},
  file = {/Users/yiyangwang/Zotero/storage/8UE6U89G/Guo et al. - 2017 - LX2343 alleviates cognitive impairments in AD mode.pdf}
}

@article{guoLX2343AlleviatesCognitive2017a,
  title = {{{LX2343}} Alleviates Cognitive Impairments in {{AD}} Model Rats by Inhibiting Oxidative Stress-Induced Neuronal Apoptosis and Tauopathy},
  author = {Guo, Xiao-dan and Sun, Guang-long and Zhou, Ting-ting and Wang, Yi-yang and Xu, Xin and Shi, Xiao-fan and Zhu, Zhi-yuan and Rukachaisirikul, Vatcharin and Hu, Li-hong and Shen, Xu},
  year = {2017},
  month = aug,
  journal = {Acta Pharmacologica Sinica},
  volume = {38},
  number = {8},
  pages = {1104--1119},
  issn = {1745-7254},
  doi = {10.1038/aps.2016.128},
  urldate = {2024-12-30},
  abstract = {Alzheimer's disease (AD) is a progressive neurodegenerative disease leading to the irreversible loss of brain neurons and cognitive abilities, and the vicious interplay between oxidative stress (OS) and tauopathy is believed to be one of the major players in AD development. Here, we demonstrated the capability of the small molecule N-(1,3-benzodioxol-5-yl)-2-[5-chloro-2-methoxy(phenylsulfonyl)anilino]acetamide (LX2343) to ameliorate the cognitive dysfunction of AD model rats by inhibiting OS-induced neuronal apoptosis and tauopathy. Streptozotocin (STZ) was used to induce OS in neuronal cells in vitro and in AD model rats that were made by intracerebroventricular injection of STZ (3 mg/kg, bilaterally), and Morris water maze test was used to evaluate the cognitive dysfunction in ICV-STZ rats. Treatment with LX2343 (5--20 {$\mu$}mol/L) significantly attenuated STZ-induced apoptosis in SH-SY5Y cells and mouse primary cortical neurons by alleviating OS and inhibiting the JNK/p38 and pro-apoptotic pathways. LX2343 was able to restore the integrity of mitochondrial function and morphology, increase ATP biosynthesis, and reduce ROS accumulation in the neuronal cells. In addition, LX2343 was found to be a non-ATP competitive GSK-3{$\beta$} inhibitor with IC50 of 1.84{\textpm}0.07 {$\mu$}mol/L, and it potently inhibited tau hyperphosphorylation in the neuronal cells. In ICV-STZ rats, administration of LX2343 (7, 21 mg{$\cdot$}kg-1{$\cdot$}d-1, ip, for 5 weeks) efficiently improved their cognitive deficits. LX2343 ameliorates the cognitive dysfunction in the AD model rats by suppressing OS-induced neuronal apoptosis and tauopathy, thus highlighting the potential of LX2343 for the treatment of AD.},
  keywords = {Biomedicine,general,Immunology,Internal Medicine,Medical Microbiology,Pharmacology/Toxicology,Vaccine},
  file = {/Users/yiyangwang/Zotero/storage/TEKJVXFZ/Guo et al. - 2017 - LX2343 alleviates cognitive impairments in AD mode.pdf}
}

@inproceedings{hamalainenAnalyzingPokemonMario2024,
  title = {Analyzing {{Pok{\'e}mon}} and {{Mario Streamers}}' {{Twitch Chat}} with {{LLM-based User Embeddings}}},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Natural Language Processing}} for {{Digital Humanities}}},
  author = {H{\"a}m{\"a}l{\"a}inen, Mika and Rueter, Jack and Alnajjar, Khalid},
  editor = {H{\"a}m{\"a}l{\"a}inen, Mika and {\"O}hman, Emily and Miyagawa, So and Alnajjar, Khalid and Bizzoni, Yuri},
  year = {2024},
  month = nov,
  pages = {499--503},
  publisher = {Association for Computational Linguistics},
  address = {Miami, USA},
  doi = {10.18653/v1/2024.nlp4dh-1.48},
  urldate = {2025-05-26},
  abstract = {We present a novel digital humanities method for representing our Twitch chatters as user embeddings created by a large language model (LLM). We cluster these embeddings automatically using affinity propagation and further narrow this clustering down through manual analysis. We analyze the chat of one stream by each Twitch streamer: SmallAnt, DougDoug and PointCrow. Our findings suggest that each streamer has their own type of chatters, however two categories emerge for all of the streamers: supportive viewers and emoji and reaction senders. Repetitive message spammers is a shared chatter category for two of the streamers.},
  file = {/Users/yiyangwang/Zotero/storage/MQP68GNZ/Hämäläinen et al. - 2024 - Analyzing Pokémon and Mario Streamers' Twitch Chat with LLM-based User Embeddings.pdf}
}

@inproceedings{haunschildBridgingCrisisEveryday2021,
  title = {Bridging from {{Crisis}} to {{Everyday Life}} -- {{An Analysis}} of {{User Reviews}} of the {{Warning App NINA}} and the {{COVID-19 Regulation Apps CoroBuddy}} and {{DarfIchDas}}},
  booktitle = {Companion {{Publication}} of the 2021 {{Conference}} on {{Computer Supported Cooperative Work}} and {{Social Computing}}},
  author = {Haunschild, Jasmin and Reuter, Christian},
  year = {2021},
  month = oct,
  series = {{{CSCW}} '21 {{Companion}}},
  pages = {72--78},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3462204.3481745},
  urldate = {2025-02-12},
  abstract = {During a dynamic and protracted crisis such as the COVID-19 pandemic, citizens are continuously challenged with making decisions under uncertainty. In addition to evaluating the risk of their behaviors to themselves and others, citizens also have to consider the most current regulation, which often varies federally and locally and by incidence numbers. Few tools help to stay informed about the current rules. The state-run German multi-hazard warning app NINA incorporated a feature for COVID-19, while two apps, DarfIchDas and CoroBuddy, focus only on COVID-19 regulation and are privately run. To investigate users' expectations, perceived advantages, and gaps as well as the developers' challenges, we analyze recent app store reviews of the apps and developers' replies. We show that the warning app and the COVID-19 regulation apps are evaluated on different terms, that the correctness and portrayal of complex rules are the main challenges and that developers and editors are underusing users' potential for crowdsourcing.},
  isbn = {978-1-4503-8479-7},
  file = {/Users/yiyangwang/Zotero/storage/RXT2LB4F/Haunschild and Reuter - 2021 - Bridging from Crisis to Everyday Life – An Analysi.pdf}
}

@inproceedings{heldXVARSIntroducingExplainability2024,
  title = {X-{{VARS}}: {{Introducing Explainability}} in {{Football Refereeing}} with {{Multi-Modal Large Language Models}}},
  shorttitle = {X-{{VARS}}},
  booktitle = {2024 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Held, Jan and Itani, Hani and Cioppa, Anthony and Giancola, Silvio and Ghanem, Bernard and Van Droogenbroeck, Marc},
  year = {2024},
  month = jun,
  pages = {3267--3279},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPRW63382.2024.00332},
  urldate = {2025-05-26},
  abstract = {The rapid advancement of artificial intelligence has led to significant improvements in automated decision-making. However, the increased performance of models often comes at the cost of explainability and transparency of their decision-making processes. In this paper, we investigate the capabilities of large language models to explain decisions, using football refereeing as a testing ground, given its decision complexity and subjectivity. We introduce the EXplainable Video Assistant Referee System, X-VARS, a multi-modal large language model designed for understanding football videos from the point of view of a referee. X-VARS can perform a multitude of tasks, including video description, question answering, action recognition, and conducting meaningful conversations based on video content and in accordance with the Laws of the Game for football referees. We validate X-VARS on our novel dataset, SoccerNet-XFoul, which consists of more than 22k videoquestion-answer triplets annotated by over 70 experienced football referees. Our experiments and human study illustrate the impressive capabilities of X-VARS in interpreting complex football clips. Furthermore, we highlight the potential of X-VARS to reach human performance and support football referees in the future. We will provide code, model, dataset, and demo upon publication.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-6547-4},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/UPNSHFUM/Held et al. - 2024 - X-VARS Introducing Explainability in Football Refereeing with Multi-Modal Large Language Models.pdf}
}

@inproceedings{huangElicitingDesignGuidelines2025,
  title = {Eliciting {{Design Guidelines}} of {{Paper-Based Tactile Interfaces}} for {{Eyes-Free Scenarios}}},
  booktitle = {Proceedings of the {{Nineteenth International Conference}} on {{Tangible}}, {{Embedded}}, and {{Embodied Interaction}}},
  author = {Huang, Han and Cheng, Tingyu and Ye, Jinzhi and Zhao, Yuhui and Wang, Yiyang and Xu, Haiqing and Stangl, Abigale and Oh, HyunJoo},
  year = {2025},
  month = mar,
  pages = {1--15},
  publisher = {ACM},
  address = {Bordeaux/Talence France},
  doi = {10.1145/3689050.3704930},
  urldate = {2025-07-08},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  file = {/Users/yiyangwang/Zotero/storage/XG3L3ZBY/Huang et al. - 2025 - Eliciting Design Guidelines of Paper-Based Tactile Interfaces for Eyes-Free Scenarios.pdf}
}

@article{jainColloSSLCollaborativeSelfSupervised2022,
  title = {{{ColloSSL}}: {{Collaborative Self-Supervised Learning}} for {{Human Activity Recognition}}},
  shorttitle = {{{ColloSSL}}},
  author = {Jain, Yash and Tang, Chi Ian and Min, Chulhong and Kawsar, Fahim and Mathur, Akhil},
  year = {2022},
  month = mar,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {6},
  number = {1},
  pages = {1--28},
  issn = {2474-9567},
  doi = {10.1145/3517246},
  urldate = {2025-04-02},
  abstract = {A major bottleneck in training robust Human-Activity Recognition models (HAR) is the need for large-scale labeled sensor datasets. Because labeling large amounts of sensor data is an expensive task, unsupervised and semi-supervised learning techniques have emerged that can learn good features from the data without requiring any labels. In this paper, we extend this line of research and present a novel technique called Collaborative Self-Supervised Learning (ColloSSL) which leverages unlabeled data collected from multiple devices worn by a user to learn high-quality features of the data. A key insight that underpins the design of ColloSSL is that unlabeled sensor datasets simultaneously captured by multiple devices can be viewed as natural transformations of each other, and leveraged to generate a supervisory signal for representation learning. We present three technical innovations to extend conventional self-supervised learning algorithms to a multi-device setting: a Device Selection approach which selects positive and negative devices to enable contrastive learning, a Contrastive Sampling algorithm which samples positive and negative examples in a multi-device setting, and a loss function called Multi-view Contrastive Loss which extends standard contrastive loss to a multi-device setting. Our experimental results on three multi-device datasets show that ColloSSL outperforms both fully-supervised and semi-supervised learning techniques in majority of the experiment settings, resulting in an absolute increase of upto 7.9\% in F1 score compared to the best performing baselines. We also show that ColloSSL outperforms the fully-supervised methods in a low-data regime, by just using one-tenth of the available labeled data in the best case.},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/T9UNF4CM/Jain et al. - 2022 - ColloSSL Collaborative Self-Supervised Learning for Human Activity Recognition.pdf}
}

@inproceedings{jansenAutoVisEnablingMixedImmersive2023,
  title = {{{AutoVis}}: {{Enabling Mixed-Immersive Analysis}} of {{Automotive User Interface Interaction Studies}}},
  shorttitle = {{{AutoVis}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Jansen, Pascal and Britten, Julian and H{\"a}usele, Alexander and Segschneider, Thilo and Colley, Mark and Rukzio, Enrico},
  year = {2023},
  month = apr,
  pages = {1--23},
  publisher = {ACM},
  address = {Hamburg Germany},
  doi = {10.1145/3544548.3580760},
  urldate = {2025-05-13},
  isbn = {978-1-4503-9421-5},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/EX237TH6/Jansen et al. - 2023 - AutoVis Enabling Mixed-Immersive Analysis of Automotive User Interface Interaction Studies.pdf}
}

@inproceedings{jinAgentReviewExploringPeer2024,
  title = {{{AgentReview}}: {{Exploring Peer Review Dynamics}} with {{LLM Agents}}},
  shorttitle = {{{AgentReview}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Jin, Yiqiao and Zhao, Qinlin and Wang, Yiyang and Chen, Hao and Zhu, Kaijie and Xiao, Yijia and Wang, Jindong},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {1208--1226},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.70},
  urldate = {2024-12-30},
  abstract = {Peer review is fundamental to the integrity and advancement of scientific publication. Traditional methods of peer review analyses often rely on exploration and statistics of existing peer review data, which do not adequately address the multivariate nature of the process, account for the latent variables, and are further constrained by privacy concerns due to the sensitive nature of the data. We introduce AgentReview, the first large language model (LLM) based peer review simulation framework, which effectively disentangles the impacts of multiple latent factors and addresses the privacy issue. Our study reveals significant insights, including a notable 37.1\% variation in paper decisions due to reviewers' biases, supported by sociological theories such as the social influence theory, altruism fatigue, and authority bias. We believe that this study could offer valuable insights to improve the design of peer review mechanisms.},
  file = {/Users/yiyangwang/Zotero/storage/DK79ELVK/Jin et al. - 2024 - AgentReview Exploring Peer Review Dynamics with L.pdf;/Users/yiyangwang/Zotero/storage/VX3UA4II/Jin et al. - 2024 - AgentReview Exploring Peer Review Dynamics with L.pdf}
}

@inproceedings{jinAgentReviewExploringPeer2024a,
  title = {{{AgentReview}}: {{Exploring Peer Review Dynamics}} with {{LLM Agents}}},
  shorttitle = {{{AgentReview}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Jin, Yiqiao and Zhao, Qinlin and Wang, Yiyang and Chen, Hao and Zhu, Kaijie and Xiao, Yijia and Wang, Jindong},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {1208--1226},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.70},
  urldate = {2024-12-30},
  abstract = {Peer review is fundamental to the integrity and advancement of scientific publication. Traditional methods of peer review analyses often rely on exploration and statistics of existing peer review data, which do not adequately address the multivariate nature of the process, account for the latent variables, and are further constrained by privacy concerns due to the sensitive nature of the data. We introduce AgentReview, the first large language model (LLM) based peer review simulation framework, which effectively disentangles the impacts of multiple latent factors and addresses the privacy issue. Our study reveals significant insights, including a notable 37.1\% variation in paper decisions due to reviewers' biases, supported by sociological theories such as the social influence theory, altruism fatigue, and authority bias. We believe that this study could offer valuable insights to improve the design of peer review mechanisms.},
  file = {/Users/yiyangwang/Zotero/storage/AKGAX9JR/Jin et al. - 2024 - AgentReview Exploring Peer Review Dynamics with L.pdf}
}

@article{jinScito2M2Million2024,
  title = {{{Scito2M}}: {{A}} 2 {{Million}}, 30-{{Year Cross-disciplinary Dataset}} for {{Temporal Scientometric Analysis}}},
  shorttitle = {{{Scito2M}}},
  author = {Jin, Yiqiao and Xiao, Yijia and Wang, Yiyang and Wang, Jindong},
  year = {2024},
  month = oct,
  journal = {(Best Paper Award) Good Data @ AAAI 2025 Workshop},
  eprint = {2410.09510},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2410.09510},
  urldate = {2024-12-30},
  abstract = {Understanding the creation, evolution, and dissemination of scientific knowledge is crucial for bridging diverse subject areas and addressing complex global challenges such as pandemics, climate change, and ethical AI. Scientometrics, the quantitative and qualitative study of scientific literature, provides valuable insights into these processes. We introduce Scito2M, a longitudinal scientometric dataset with over two million academic publications, providing comprehensive contents information and citation graphs to support cross-disciplinary analyses. Using Scito2M, we conduct a temporal study spanning over 30 years to explore key questions in scientometrics: the evolution of academic terminology, citation patterns, and interdisciplinary knowledge exchange. Our findings reveal critical insights, such as disparities in epistemic cultures, knowledge production modes, and citation practices. For example, rapidly developing, application-driven fields like LLMs exhibit significantly shorter citation age (2.48 years) compared to traditional theoretical disciplines like oral history (9.71 years).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Digital Libraries},
  file = {/Users/yiyangwang/Zotero/storage/LRTBE555/Jin et al. - 2024 - Scito2M A 2 Million, 30-Year Cross-disciplinary D.pdf;/Users/yiyangwang/Zotero/storage/6VAUMDNS/2410.html}
}

@misc{jinScito2M2Million2024a,
  title = {{{Scito2M}}: {{A}} 2 {{Million}}, 30-{{Year Cross-disciplinary Dataset}} for {{Temporal Scientometric Analysis}}},
  shorttitle = {{{Scito2M}}},
  author = {Jin, Yiqiao and Xiao, Yijia and Wang, Yiyang and Wang, Jindong},
  year = {2024},
  month = oct,
  number = {arXiv:2410.09510},
  eprint = {2410.09510},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.09510},
  urldate = {2024-12-30},
  abstract = {Understanding the creation, evolution, and dissemination of scientific knowledge is crucial for bridging diverse subject areas and addressing complex global challenges such as pandemics, climate change, and ethical AI. Scientometrics, the quantitative and qualitative study of scientific literature, provides valuable insights into these processes. We introduce Scito2M, a longitudinal scientometric dataset with over two million academic publications, providing comprehensive contents information and citation graphs to support cross-disciplinary analyses. Using Scito2M, we conduct a temporal study spanning over 30 years to explore key questions in scientometrics: the evolution of academic terminology, citation patterns, and interdisciplinary knowledge exchange. Our findings reveal critical insights, such as disparities in epistemic cultures, knowledge production modes, and citation practices. For example, rapidly developing, application-driven fields like LLMs exhibit significantly shorter citation age (2.48 years) compared to traditional theoretical disciplines like oral history (9.71 years).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Digital Libraries},
  file = {/Users/yiyangwang/Zotero/storage/3MVIURKV/Jin et al. - 2024 - Scito2M A 2 Million, 30-Year Cross-disciplinary D.pdf}
}

@inproceedings{kimBleacherBotAIAgent2025,
  title = {{{BleacherBot}}: {{AI Agent}} as a {{Sports Co-Viewing Partner}}},
  shorttitle = {{{BleacherBot}}},
  booktitle = {Proceedings of the 2025 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kim, Kyusik and Song, Hyungwoo and Ryu, Jeongwoo and Oh, Changhoon and Suh, Bongwon},
  year = {2025},
  month = apr,
  pages = {1--31},
  publisher = {ACM},
  address = {Yokohama Japan},
  doi = {10.1145/3706598.3714178},
  urldate = {2025-05-05},
  isbn = {979-8-4007-1394-1},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/HR9SQ2IS/Kim et al. - 2025 - BleacherBot AI Agent as a Sports Co-Viewing Partner.pdf}
}

@misc{kryderHowUseOura2023,
  title = {How to {{Use Oura Temperature Trends}} to {{Track Your Cycle}}},
  author = {Kryder, Caroline},
  year = {2023},
  month = oct,
  journal = {The Pulse Blog},
  urldate = {2025-02-05},
  abstract = {Your body temperature naturally shifts throughout your cycle, and Oura Ring can be a helpful tool for tracking it and noticing changes.},
  howpublished = {https://ouraring.com/blog/temperature-to-track-your-menstrual-cycle/},
  langid = {american}
}

@inproceedings{lanLLMBasedAgentSociety2024,
  title = {{{LLM-Based Agent Society Investigation}}: {{Collaboration}} and {{Confrontation}} in {{Avalon Gameplay}}},
  shorttitle = {{{LLM-Based Agent Society Investigation}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Lan, Yihuai and Hu, Zhiqiang and Wang, Lei and Wang, Yang and Ye, Deheng and Zhao, Peilin and Lim, Ee-Peng and Xiong, Hui and Wang, Hao},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {128--145},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.7},
  urldate = {2025-05-14},
  abstract = {This paper explores the open research problem of understanding the social behaviors of LLM-based agents. Using Avalon as a testbed, we employ system prompts to guide LLM agents in gameplay. While previous studies have touched on gameplay with LLM agents, research on their social behaviors is lacking. We propose a novel framework, tailored for Avalon, features a multi-agent system facilitating efficient communication and interaction. We evaluate its performance based on game success and analyze LLM agents' social behaviors. Results affirm the framework`s effectiveness in creating adaptive agents and suggest LLM-based agents' potential in navigating dynamic social interactions. By examining collaboration and confrontation behaviors, we offer insights into this field`s research and applications.},
  langid = {american},
  file = {/Users/yiyangwang/Zotero/storage/27J3Q5WS/Lan et al. - 2024 - LLM-Based Agent Society Investigation Collaboration and Confrontation in Avalon Gameplay.pdf}
}

@article{liBIG5CHATShapingLLM2024,
  title = {{{BIG5-CHAT}}: {{Shaping LLM Personalities Through Training}} on {{Human-Grounded Data}}},
  shorttitle = {{{BIG5-CHAT}}},
  author = {Li, Wenkai and Liu, Jiarui and Liu, Andy and Zhou, Xuhui and Diab, Mona T. and Sap, Maarten},
  year = {2024},
  month = oct,
  urldate = {2025-05-27},
  abstract = {In this work, we tackle the challenge of embedding realistic human personality traits into LLMs. Previous approaches have primarily focused on prompt-based methods that describe the behavior associated with the desired personality traits, suffering from realism and validity issues. To address these limitations, we introduce BIG5-CHAT, a large-scale dataset containing 100,000 dialogues designed to ground models in how humans express their personality in text. Leveraging this dataset, we explore Supervised Fine-Tuning and Direct Preference Optimization as training-based methods to align LLMs more naturally with human personality patterns. Our methods outperform prompting on personality assessments such as BFI and IPIP-NEO, with trait correlations more closely matching human data. Furthermore, our experiments reveal that models trained to exhibit higher conscientiousness, higher agreeableness, lower extraversion, and lower neuroticism display better performance on reasoning tasks, aligning with psychological findings on how these traits impact human cognitive performance. To our knowledge, this work is the first comprehensive study to demonstrate how training-based methods can shape LLM personalities through learning from real human behaviors.},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/ME8IPEBM/Li et al. - 2024 - BIG5-CHAT Shaping LLM Personalities Through Training on Human-Grounded Data.pdf}
}

@article{linQuestOmniocularsEmbedded2023,
  title = {The {{Quest}} for {{Omnioculars}}: {{Embedded Visualization}} for {{Augmenting Basketball Game Viewing Experiences}}},
  shorttitle = {The {{Quest}} for {{Omnioculars}}},
  author = {Lin, Tica and {Zhu-Tian}, Chen and Yang, Yalong and Chiappalupi, Daniele and Beyer, Johanna and Pfister, Hanspeter},
  year = {2023},
  month = jan,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {29},
  number = {1},
  pages = {962--972},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2022.3209353},
  urldate = {2025-05-25},
  abstract = {Sports game data is becoming increasingly complex, often consisting of multivariate data such as player performance stats, historical team records, and athletes' positional tracking information. While numerous visual analytics systems have been developed for sports analysts to derive insights, few tools target fans to improve their understanding and engagement of sports data during live games. By presenting extra data in the actual game views, embedded visualization has the potential to enhance fans' game-viewing experience. However, little is known about how to design such kinds of visualizations embedded into live games. In this work, we present a user-centered design study of developing interactive embedded visualizations for basketball fans to improve their live game-watching experiences. We first conducted a formative study to characterize basketball fans' in-game analysis behaviors and tasks. Based on our findings, we propose a design framework to inform the design of embedded visualizations based on specific data-seeking contexts. Following the design framework, we present five novel embedded visualization designs targeting five representative contexts identified by the fans, including shooting, offense, defense, player evaluation, and team comparison. We then developed Omnioculars, an interactive basketball game-viewing prototype that features the proposed embedded visualizations for fans' in-game data analysis. We evaluated Omnioculars in a simulated basketball game with basketball fans. The study results suggest that our design supports personalized in-game data analysis and enhances game understanding and engagement.},
  langid = {american},
  keywords = {,Data analysis,Data visualization,Data Visualization,Embedded Visualization,Fans,Games,Sports,Sports Analytics,Videos,Visual analytics},
  file = {/Users/yiyangwang/Zotero/storage/TPW6TA7W/Lin et al. - 2023 - The Quest for Omnioculars Embedded Visualization for Augmenting Basketball Game Viewing Experiences.pdf}
}

@inproceedings{luUnpackingLivedExperience2024,
  title = {Unpacking the {{Lived Experience}} of {{Collaborative Pregnancy Tracking}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Lu, Xi and Powell, Jacquelyn E and Agapie, Elena and Chen, Yunan and Epstein, Daniel A.},
  year = {2024},
  month = may,
  pages = {1--17},
  publisher = {ACM},
  address = {Honolulu HI USA},
  doi = {10.1145/3613904.3642652},
  urldate = {2025-02-09},
  abstract = {Pregnancy brings physical, emotional, and economic challenges for expectant parent(s), close relatives, and friends. Existing technology support, including tracking technology, largely targets pregnant people and ignores other stakeholders. We therefore lack an understanding of how to approach designing collaborative pregnancy tracking technology. To understand how people collaborate around pregnancy tracking and wish to do so, we interviewed 13 pregnant people and 11 non-pregnant stakeholders in the U.S., including partners, friends, and grandparents-to-be. We find that people collaborate for goals like social bonding and jointly managing various pregnancy data. Stakeholders collaborated by either dividing up data types or collectively monitoring the same information. We also identify tensions and challenges, such as pregnant people's privacy concerns and stakeholders' varied levels of interest in tracking. In light of socio-cultural norms and stakeholders' distinctive roles around pregnancy, we point to opportunities for designing collaborative technology that aligns with as well as challenges socio-cultural practices around pregnancy tracking.},
  isbn = {979-8-4007-0330-0},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/Y8FY3KHC/Lu et al. - 2024 - Unpacking the Lived Experience of Collaborative Pr.pdf}
}

@article{madhvapathyReliableLowcostFully2020,
  title = {Reliable, Low-Cost, Fully Integrated Hydration Sensors for Monitoring and Diagnosis of Inflammatory Skin Diseases in Any Environment},
  author = {Madhvapathy, Surabhi R. and Wang, Heling and Kong, Jessy and Zhang, Michael and Lee, Jong Yoon and Park, Jun Bin and Jang, Hokyung and Xie, Zhaoqian and Cao, Jingyue and Avila, Raudel and Wei, Chen and D'Angelo, Vincent and Zhu, Jason and Chung, Ha Uk and Coughlin, Sarah and Patel, Manish and Winograd, Joshua and Lim, Jaeman and Banks, Anthony and Xu, Shuai and Huang, Yonggang and Rogers, John A.},
  year = {2020},
  month = dec,
  journal = {Science Advances},
  volume = {6},
  number = {49},
  pages = {eabd7146},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/sciadv.abd7146},
  urldate = {2025-02-05},
  abstract = {Present-day dermatological diagnostic tools are expensive, time-consuming, require substantial operational expertise, and typically probe only the superficial layers of skin ({\textasciitilde}15 {$\mu$}m). We introduce a soft, battery-free, noninvasive, reusable skin hydration sensor (SHS) adherable to most of the body surface. The platform measures volumetric water content (up to {\textasciitilde}1 mm in depth) and wirelessly transmits data to any near-field communication--compatible smartphone. The SHS is readily manufacturable, comprises unique powering and encapsulation strategies, and achieves high measurement precision ({\textpm}5\% volumetric water content) and resolution ({\textpm}0.015{$^\circ$}C skin surface temperature). Validation on n = 16 healthy/normal human participants reveals an average skin water content of {\textasciitilde}63\% across multiple body locations. Pilot studies on patients with atopic dermatitis (AD), psoriasis, urticaria, xerosis cutis, and rosacea highlight the diagnostic capability of the SHS (PAD = 0.0034) and its ability to study impact of topical treatments on skin diseases.},
  file = {/Users/yiyangwang/Zotero/storage/XE7FH5XM/Madhvapathy et al. - 2020 - Reliable, low-cost, fully integrated hydration sen.pdf}
}

@article{mukherjeeInformationSharingViewers2017,
  title = {Information {{Sharing}} by {{Viewers Via Second Screens}} for {{In-Real-Life Events}}},
  author = {Mukherjee, Partha and Jansen, Bernard J.},
  year = {2017},
  month = mar,
  journal = {ACM Trans. Web},
  volume = {11},
  number = {1},
  pages = {1:1--1:24},
  issn = {1559-1131},
  doi = {10.1145/3009970},
  urldate = {2025-05-15},
  abstract = {The use of second screen devices with social media facilitates conversational interaction concerning broadcast media events, creating what we refer to as the social soundtrack. In this research, we evaluate the change of the Super Bowl XLIX social soundtrack across three social media platforms on the topical categories of commercials, music, and game at three game phases (Pre, During, and Post). We perform statistical analysis on more than 3M, 800K, and 50K posts from Twitter, Instagram, and Tumblr, respectively. Findings show that the volume of posts in the During phase is fewer compared to Pre and Post phases; however, the hourly mean in the During phase is considerably higher than it is in the other two phases. We identify the predominant phase and category of interaction across all three social media sites. We also determine the significance of change in absolute scale across the Super Bowl categories (commercials, music, game) and in both absolute and relative scales across Super Bowl phases (Pre, During, Post) for the three social network platforms (Twitter, Tumblr, Instagram). Results show that significant phase-category relationships exist for all three social networks. The results identify the During phase as the predominant one for all three categories on all social media sites with respect to the absolute volume of conversations in a continuous scale. From the relative volume perspective, the During phase is highest for the music category for most social networks. For the commercials and game categories, however, the Post phase is higher than the During phase for Twitter and Instagram, respectively. Regarding category identification, the game category is the highest for Twitter and Instagram but not for Tumblr, which has dominant peaks for music and/or commercials in all three phases. It is apparent that different social media platforms offer various phase and category affordances. These results are important in identifying the influence that second screen technology has on information sharing across different social media platforms and indicates that the viewer role is transitioning from passive to more active.},
  langid = {american},
  file = {/Users/yiyangwang/Zotero/storage/X8CJS9EM/Mukherjee and Jansen - 2017 - Information Sharing by Viewers Via Second Screens for In-Real-Life Events.pdf}
}

@inproceedings{namvarpourUncoveringContradictionsHumanAI2024,
  title = {Uncovering {{Contradictions}} in {{Human-AI Interactions}}: {{Lessons Learned}} from {{User Reviews}} of {{Replika}}},
  shorttitle = {Uncovering {{Contradictions}} in {{Human-AI Interactions}}},
  booktitle = {Companion {{Publication}} of the 2024 {{Conference}} on {{Computer-Supported Cooperative Work}} and {{Social Computing}}},
  author = {Namvarpour, Mohammad and Razi, Afsaneh},
  year = {2024},
  month = nov,
  series = {{{CSCW Companion}} '24},
  pages = {579--586},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3678884.3681909},
  urldate = {2025-02-12},
  abstract = {The increasing integration of artificial intelligence (AI) in daily life presents opportunities and challenges, particularly in fostering human-AI relationships. This study investigates user reviews from the Google Play Store for the Replika chatbot, focusing on complaints of online sexual harassment. Using Activity Theory, the analysis reveals several contradictions within the Replika activity system, including tool-subject, tool-object, rule-subject, rule-object, and distribution of labor-subject and labor-object contradictions. These contradictions highlight the misalignment between user expectations and the chatbot's behavior, inadequate safety measures, and unrealistic expectations placed on users to train the AI. The study emphasizes the need for clearer objectives, advanced AI alignment techniques, and refined safety protocols to enhance the user experience and ensure ethical interactions with chatbots. We provide implications for guidelines on the development of more trustworthy and supportive digital companions.},
  isbn = {979-8-4007-1114-5},
  file = {/Users/yiyangwang/Zotero/storage/Z5ZX4JGA/Namvarpour and Razi - 2024 - Uncovering Contradictions in Human-AI Interactions.pdf}
}

@inproceedings{nowakHearWeAre2023,
  title = {Hear {{We Are}}: {{Spatial Audio Benefits Perceptions}} of {{Turn-Taking}} and {{Social Presence}} in {{Video Meetings}}},
  shorttitle = {Hear {{We Are}}},
  booktitle = {Proceedings of the 2nd {{Annual Meeting}} of the {{Symposium}} on {{Human-Computer Interaction}} for {{Work}}},
  author = {Nowak, Kate and Tankelevitch, Lev and Tang, John and Rintel, Sean},
  year = {2023},
  month = jun,
  pages = {1--10},
  publisher = {ACM},
  address = {Oldenburg Germany},
  doi = {10.1145/3596671.3598578},
  urldate = {2025-05-14},
  abstract = {Relative to in-person meetings, conversations in video meetings have long been reported as stilted. Spatial audio in video meetings can simulate the way we hear the world by separating audio streams based on speakers' virtual locations. We report on a within-subject experiment in which 75 employees of a global technology company completed two group survival tasks with spatial audio enabled or disabled. Spatial audio increased perceptions of interactivity, shared space, and ease of understanding. Women experienced effects for social presence while men experienced effects for turn-taking. We discuss implications for inclusion, task performance, fatigue, and future research.},
  isbn = {979-8-4007-0807-7},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/ZEDTQ72P/Nowak et al. - 2023 - Hear We Are Spatial Audio Benefits Perceptions of Turn-Taking and Social Presence in Video Meetings.pdf}
}

@misc{PhDCSHumanComputer,
  title = {Ph.{{D}}. {{CS Human-Computer Interaction Body}} of {{Knowledge}} {\textbar} {{College}} of {{Computing}}},
  urldate = {2024-10-22},
  howpublished = {https://www.cc.gatech.edu/phd-cs-human-computer-interaction-body-knowledge},
  file = {/Users/yiyangwang/Zotero/storage/AMS28H4D/phd-cs-human-computer-interaction-body-knowledge.html}
}

@article{philipsEmpoweringSeniorCochlear2018,
  title = {Empowering {{Senior Cochlear Implant Users}} at {{Home}} via a {{Tablet Computer Application}}},
  author = {Philips, Birgit and Smits, Cas and Govaerts, Paul J. and Doorn, Inge and Vanpoucke, Filiep},
  year = {2018},
  month = nov,
  journal = {American Journal of Audiology},
  volume = {27},
  number = {3S},
  pages = {417--430},
  publisher = {American Speech-Language-Hearing Association},
  doi = {10.1044/2018_AJA-IMIA3-18-0014},
  urldate = {2025-02-07},
  abstract = {PurposeThe introduction of connectivity technologies in hearing implants allows new ways to support cochlear implant (CI) users remotely. Some functionalities and services that are traditionally only available in an in-clinic care model can now also be accessed at home. This study explores the feasibility of a prototype of a tablet computer application (MyHearingApp [MHA]) in a group of senior experienced CI users at home, evaluating usability and user motivation.MethodBased on user feedback, a tablet computer application (MHA) for the Cochlear Nucleus 6 CP910 sound processor was designed implementing six different functionalities: (a) My Hearing Tests, (b) My Environment, (c) My Hearing Journey, (d) Tip of the Day, (e) Recipient Portal, and (f) Program Use and Events. The clinical evaluation design was a prospective study of the MHA in 16 senior experienced CI users. During 4 weeks, participants could freely explore the functionalities. At the end, the usability and their motivation for uptake and adherence were measured using a baseline and follow-up questionnaire.ResultsBased on the System Usability Score (as part of the follow-up questionnaire), a good level of usability was indicated (M = 75.6, range: 62.5--92.5, SD = 8.6). The ability to perform hearing tests at home is ranked as the most relevant functionality within the MHA. According to the Intrinsic Motivation Inventory (Deci, Eghrari, Patrick, \& Leone, 1994) questionnaire (as part of the follow-up questionnaire), participants reported high levels of interest and enjoyment, found themselves competent, and did not experience pressure while working with the app.ConclusionsThis study evaluated a tablet computer application (MHA) for experienced senior CI users by means of a prospective design, which provided novel insights into delivering CI care into the home of the CI user. The user feedback from this small-scale study suggests that the participants are open to take more responsibility for and to become a more active actor in their own hearing care, if only this is facilitated with the right tools. This may foster the evolution from a clinic-led to a more patient-centered care model, where CI users feel more empowered in the self-management of their hearing implant device.},
  file = {/Users/yiyangwang/Zotero/storage/6U8DI5MU/Philips et al. - 2018 - Empowering Senior Cochlear Implant Users at Home v.pdf}
}

@article{quesadaUseBallistocardiographyMonitor2021,
  title = {Use of {{Ballistocardiography}} to {{Monitor Cardiovascular Hemodynamics}} in {{Preeclampsia}}},
  author = {Quesada, Odayme and Shandhi, Md Mobashir Hasan and Beach, Shire and Dowling, Sean and Tandon, Damini and Heller, James and Etemadi, Mozziyar and Roy, Shuvo and Velez, Juan M. Gonzalez and Inan, Omer T. and Klein, Liviu},
  year = {2021},
  month = apr,
  journal = {Women's Health Reports},
  volume = {2},
  number = {1},
  pages = {97},
  doi = {10.1089/whr.2020.0127},
  urldate = {2024-11-12},
  abstract = {Objective: Pregnancy requires a complex physiological adaptation of the maternal cardiovascular system, which is disrupted in women with pregnancies complicated by preeclampsia, putting them at higher risk of future cardiovascular events. The ...},
  langid = {english},
  pmid = {33937907},
  file = {/Users/yiyangwang/Zotero/storage/SQPRLECA/Quesada et al. - 2021 - Use of Ballistocardiography to Monitor Cardiovascu.pdf}
}

@misc{raoMultiAgentSystemComprehensive2025,
  title = {Multi-{{Agent System}} for {{Comprehensive Soccer Understanding}}},
  author = {Rao, Jiayuan and Li, Zifeng and Wu, Haoning and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
  year = {2025},
  month = may,
  number = {arXiv:2505.03735},
  eprint = {2505.03735},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.03735},
  urldate = {2025-06-03},
  abstract = {Recent advancements in AI-driven soccer understanding have demonstrated rapid progress, yet existing research predominantly focuses on isolated or narrow tasks. To bridge this gap, we propose a comprehensive framework for holistic soccer understanding. Specifically, we make the following contributions in this paper: (i) we construct SoccerWiki, the first large-scale multimodal soccer knowledge base, integrating rich domain knowledge about players, teams, referees, and venues to enable knowledge-driven reasoning; (ii) we present SoccerBench, the largest and most comprehensive soccer-specific benchmark, featuring around 10K standardized multimodal (text, image, video) multi-choice QA pairs across 13 distinct understanding tasks, curated through automated pipelines and manual verification; (iii) we introduce SoccerAgent, a novel multi-agent system that decomposes complex soccer questions via collaborative reasoning, leveraging domain expertise from SoccerWiki and achieving robust performance; (iv) extensive evaluations and ablations that benchmark state-of-the-art MLLMs on SoccerBench, highlighting the superiority of our proposed agentic system. All data and code are publicly available at: https://jyrao.github.io/SoccerAgent/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/yiyangwang/Zotero/storage/CX7KBGFM/Rao et al. - 2025 - Multi-Agent System for Comprehensive Soccer Understanding.pdf;/Users/yiyangwang/Zotero/storage/F9WST6SY/2505.html}
}

@misc{raoMultiAgentSystemComprehensive2025a,
  title = {Multi-{{Agent System}} for {{Comprehensive Soccer Understanding}}},
  author = {Rao, Jiayuan and Li, Zifeng and Wu, Haoning and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
  year = {2025},
  month = may,
  number = {arXiv:2505.03735},
  eprint = {2505.03735},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.03735},
  urldate = {2025-06-03},
  abstract = {Recent advancements in AI-driven soccer understanding have demonstrated rapid progress, yet existing research predominantly focuses on isolated or narrow tasks. To bridge this gap, we propose a comprehensive framework for holistic soccer understanding. Specifically, we make the following contributions in this paper: (i) we construct SoccerWiki, the first large-scale multimodal soccer knowledge base, integrating rich domain knowledge about players, teams, referees, and venues to enable knowledge-driven reasoning; (ii) we present SoccerBench, the largest and most comprehensive soccer-specific benchmark, featuring around 10K standardized multimodal (text, image, video) multi-choice QA pairs across 13 distinct understanding tasks, curated through automated pipelines and manual verification; (iii) we introduce SoccerAgent, a novel multi-agent system that decomposes complex soccer questions via collaborative reasoning, leveraging domain expertise from SoccerWiki and achieving robust performance; (iv) extensive evaluations and ablations that benchmark state-of-the-art MLLMs on SoccerBench, highlighting the superiority of our proposed agentic system. All data and code are publicly available at: https://jyrao.github.io/SoccerAgent/.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {,Computer Science - Computer Vision and Pattern Recognition}
}

@article{renellaAutomatedVideoGame,
  title = {Towards {{Automated Video Game Commentary Using Generative AI}}},
  author = {Renella, Noah and Eger, Markus},
  abstract = {In recent years, video game streaming has enjoyed ever-growing popularity. While streaming gameplay, a streamer is also expected to provide commentary on gameplay and otherwise entertain the audience. The mental burden for providing such commentary while also playing the game may provide a barrier of entry for new streamers. In this paper, we present an approach to automatically generate streaming commentary during gameplay for the game League of Legends. Our system recognizes key events, and uses generative AI services to generate voice output. We also present a preliminary evaluation of our system, both in quantitative terms of event recognition accuracy, as well as qualitatively by discussing the generated commentary.},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/RTM3TRVG/Renella and Eger - Towards Automated Video Game Commentary Using Generative AI.pdf}
}

@inproceedings{ryuCinemaMultiverseLounge2025,
  title = {Cinema {{Multiverse Lounge}}: {{Enhancing Film Appreciation}} via {{Multi-Agent Conversations}}},
  shorttitle = {Cinema {{Multiverse Lounge}}},
  booktitle = {Proceedings of the 2025 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Ryu, Jeongwoo and Kim, Kyusik and Heo, Dongseok and Song, Hyungwoo and Oh, Changhoon and Suh, Bongwon},
  year = {2025},
  month = apr,
  pages = {1--22},
  publisher = {ACM},
  address = {Yokohama Japan},
  doi = {10.1145/3706598.3713641},
  urldate = {2025-05-27},
  isbn = {979-8-4007-1394-1},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/VH4C9BEA/Ryu et al. - 2025 - Cinema Multiverse Lounge Enhancing Film Appreciation via Multi-Agent Conversations.pdf}
}

@misc{samuelPersonaGymEvaluatingPersona2025,
  title = {{{PersonaGym}}: {{Evaluating Persona Agents}} and {{LLMs}}},
  shorttitle = {{{PersonaGym}}},
  author = {Samuel, Vinay and Zou, Henry Peng and Zhou, Yue and Chaudhari, Shreyas and Kalyan, Ashwin and Rajpurohit, Tanmay and Deshpande, Ameet and Narasimhan, Karthik and Murahari, Vishvak},
  year = {2025},
  month = may,
  number = {arXiv:2407.18416},
  eprint = {2407.18416},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.18416},
  urldate = {2025-06-19},
  abstract = {Persona agents, which are LLM agents conditioned to act according to an assigned persona, enable contextually rich and user aligned interactions across domains like education and healthcare. However, evaluating how faithfully these agents adhere to their personas remains a significant challenge, particularly in free-form settings that demand consistency across diverse, persona-relevant environments. We introduce PersonaGym, the first dynamic evaluation framework for persona agents, and PersonaScore, a human-aligned automatic metric grounded in decision theory that enables comprehensive large-scale evaluation. Our evaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals significant advancement opportunities. For example, GPT-4.1 had the exact same PersonaScore as LLaMA-3-8b despite being a more recent and advanced closed source model. Importantly, increased model size and complexity do not necessarily enhance persona agent capabilities, underscoring the need for algorithmic and architectural innovation toward faithful, performant persona agents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiyangwang/Zotero/storage/BAH6KEP9/Samuel et al. - 2025 - PersonaGym Evaluating Persona Agents and LLMs.pdf;/Users/yiyangwang/Zotero/storage/MRF8KZ7Q/2407.html}
}

@article{sanchezEstimatingBreastMassDensity2017,
  title = {Estimating {{Breast Mass-Density}}: {{A Retrospective Analysis}} of {{Radiological Data}}},
  shorttitle = {Estimating {{Breast Mass-Density}}},
  author = {Sanchez, Amy and Mills, Chris and Scurr, Joanna},
  year = {2017},
  month = mar,
  journal = {The Breast Journal},
  volume = {23},
  number = {2},
  pages = {237--239},
  issn = {1075122X},
  doi = {10.1111/tbj.12725},
  urldate = {2025-02-12},
  copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/7KAJRMPI/Sanchez et al. - 2017 - Estimating Breast Mass-Density A Retrospective An.pdf}
}

@misc{shuEffectiveGenAIMultiAgent2024,
  title = {Towards {{Effective GenAI Multi-Agent Collaboration}}: {{Design}} and {{Evaluation}} for {{Enterprise Applications}}},
  shorttitle = {Towards {{Effective GenAI Multi-Agent Collaboration}}},
  author = {Shu, Raphael and Das, Nilaksh and Yuan, Michelle and Sunkara, Monica and Zhang, Yi},
  year = {2024},
  month = dec,
  number = {arXiv:2412.05449},
  eprint = {2412.05449},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.05449},
  urldate = {2025-06-03},
  abstract = {AI agents powered by large language models (LLMs) have shown strong capabilities in problem solving. Through combining many intelligent agents, multi-agent collaboration has emerged as a promising approach to tackle complex, multi-faceted problems that exceed the capabilities of single AI agents. However, designing the collaboration protocols and evaluating the effectiveness of these systems remains a significant challenge, especially for enterprise applications. This report addresses these challenges by presenting a comprehensive evaluation of coordination and routing capabilities in a novel multi-agent collaboration framework. We evaluate two key operational modes: (1) a coordination mode enabling complex task completion through parallel communication and payload referencing, and (2) a routing mode for efficient message forwarding between agents. We benchmark on a set of handcrafted scenarios from three enterprise domains, which are publicly released with the report. For coordination capabilities, we demonstrate the effectiveness of inter-agent communication and payload referencing mechanisms, achieving end-to-end goal success rates of 90\%. Our analysis yields several key findings: multi-agent collaboration enhances goal success rates by up to 70\% compared to single-agent approaches in our benchmarks; payload referencing improves performance on code-intensive tasks by 23\%; latency can be substantially reduced with a routing mechanism that selectively bypasses agent orchestration. These findings offer valuable guidance for enterprise deployments of multi-agent systems and advance the development of scalable, efficient multi-agent collaboration frameworks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/yiyangwang/Zotero/storage/2PW5YM5V/Shu et al. - 2024 - Towards Effective GenAI Multi-Agent Collaboration Design and Evaluation for Enterprise Applications.pdf;/Users/yiyangwang/Zotero/storage/EKYZAGTL/2412.html}
}

@misc{sugliaGoingGOALResource2022,
  title = {Going for {{GOAL}}: {{A Resource}} for {{Grounded Football Commentaries}}},
  shorttitle = {Going for {{GOAL}}},
  author = {Suglia, Alessandro and Lopes, Jos{\'e} and Bastianelli, Emanuele and Vanzo, Andrea and Agarwal, Shubham and Nikandrou, Malvina and Yu, Lu and Konstas, Ioannis and Rieser, Verena},
  year = {2022},
  month = nov,
  number = {arXiv:2211.04534},
  eprint = {2211.04534},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.04534},
  urldate = {2025-06-09},
  abstract = {Recent video+language datasets cover domains where the interaction is highly structured, such as instructional videos, or where the interaction is scripted, such as TV shows. Both of these properties can lead to spurious cues to be exploited by models rather than learning to ground language. In this paper, we present GrOunded footbAlL commentaries (GOAL), a novel dataset of football (or `soccer') highlights videos with transcribed live commentaries in English. As the course of a game is unpredictable, so are commentaries, which makes them a unique resource to investigate dynamic language grounding. We also provide state-of-the-art baselines for the following tasks: frame reordering, moment retrieval, live commentary retrieval and play-by-play live commentary generation. Results show that SOTA models perform reasonably well in most tasks. We discuss the implications of these results and suggest new tasks for which GOAL can be used. Our codebase is available at: https://gitlab.com/grounded-sport-convai/goal-baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/yiyangwang/Zotero/storage/YLB37UQL/Suglia et al. - 2022 - Going for GOAL A Resource for Grounded Football Commentaries.pdf;/Users/yiyangwang/Zotero/storage/MZC445C5/2211.html}
}

@inproceedings{terryPettingZooGymMultiAgent2021,
  title = {{{PettingZoo}}: {{Gym}} for {{Multi-Agent Reinforcement Learning}}},
  shorttitle = {{{PettingZoo}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Terry, J and Black, Benjamin and Grammel, Nathaniel and Jayakumar, Mario and Hari, Ananth and Sullivan, Ryan and Santos, Luis S and Dieffendahl, Clemens and Horsch, Caroline and {Perez-Vicente}, Rodrigo and Williams, Niall and Lokesh, Yashas and Ravi, Praveen},
  year = {2021},
  volume = {34},
  pages = {15032--15043},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-06-02},
  abstract = {This paper introduces the PettingZoo library and the accompanying Agent Environment Cycle ("AEC") games model. PettingZoo is a library of diverse sets of multi-agent environments with a universal, elegant Python API. PettingZoo was developed with the goal of accelerating research in Multi-Agent Reinforcement Learning ("MARL"), by making work more interchangeable, accessible and reproducible akin to what OpenAI's Gym library did for single-agent reinforcement learning. PettingZoo's API, while inheriting many features of Gym, is unique amongst MARL APIs in that it's based around the novel AEC games model. We argue, in part through case studies on major problems in popular MARL environments, that the popular game models are poor conceptual models of the games commonly used with MARL, that they promote severe bugs that are hard to detect, and that the AEC games model addresses these problems.},
  file = {/Users/yiyangwang/Zotero/storage/EIRQKPYK/Terry et al. - 2021 - PettingZoo Gym for Multi-Agent Reinforcement Learning.pdf}
}

@article{tongConsolidatedCriteriaReporting2007,
  title = {Consolidated Criteria for Reporting Qualitative Research ({{COREQ}}): A 32-Item Checklist for Interviews and Focus Groups},
  shorttitle = {Consolidated Criteria for Reporting Qualitative Research ({{COREQ}})},
  author = {Tong, Allison and Sainsbury, Peter and Craig, Jonathan},
  year = {2007},
  month = dec,
  journal = {International Journal for Quality in Health Care},
  volume = {19},
  number = {6},
  pages = {349--357},
  issn = {1353-4505},
  doi = {10.1093/intqhc/mzm042},
  urldate = {2025-02-09},
  abstract = {Qualitative research explores complex phenomena encountered by clinicians, health care providers, policy makers and consumers. Although partial checklists are available, no consolidated reporting framework exists for any type of qualitative design.To develop a checklist for explicit and comprehensive reporting of qualitative studies (indepth interviews and focus groups).We performed a comprehensive search in Cochrane and Campbell Protocols, Medline, CINAHL, systematic reviews of qualitative studies, author or reviewer guidelines of major medical journals and reference lists of relevant publications for existing checklists used to assess qualitative studies. Seventy-six items from 22 checklists were compiled into a comprehensive list. All items were grouped into three domains: (i) research team and reflexivity, (ii) study design and (iii) data analysis and reporting. Duplicate items and those that were ambiguous, too broadly defined and impractical to assess were removed.Items most frequently included in the checklists related to sampling method, setting for data collection, method of data collection, respondent validation of findings, method of recording data, description of the derivation of themes and inclusion of supporting quotations. We grouped all items into three domains: (i) research team and reflexivity, (ii) study design and (iii) data analysis and reporting.The criteria included in COREQ, a 32-item checklist, can help researchers to report important aspects of the research team, study methods, context of the study, findings, analysis and interpretations.},
  file = {/Users/yiyangwang/Zotero/storage/J5C55DHS/1791966.html}
}

@inproceedings{vasconcelosCountingMosquitoesWild2021,
  title = {Counting {{Mosquitoes}} in the {{Wild}}: {{An Internet}} of {{Things Approach}}},
  shorttitle = {Counting {{Mosquitoes}} in the {{Wild}}},
  booktitle = {Proceedings of the {{Conference}} on {{Information Technology}} for {{Social Good}}},
  author = {Vasconcelos, Dinarte and Yin, Myat Su and Wetjen, Fabian and Herbst, Alexander and Ziemer, Tim and F{\"o}rster, Anna and Barkowsky, Thomas and Nunes, Nuno and Haddawy, Peter},
  year = {2021},
  month = sep,
  pages = {43--48},
  publisher = {ACM},
  address = {Roma Italy},
  doi = {10.1145/3462203.3475914},
  urldate = {2025-02-10},
  abstract = {Counting mosquitoes in the wild is a crucial capability for monitoring, prediction, and control of vector-borne diseases. Current approaches are mainly manual, where specially designed mosquito traps or ovitraps are placed in areas of interest and recovered the next day. The counting itself is performed in an entomological laboratory, where individual mosquitoes are classified into species and counted. This process is costly, slow and inefficient. At the same time, mosquito counting is most relevant in tropical and sub-tropical countries, where mosquitoes spread deadly diseases like malaria, yellow fever and dengue fever. Many countries in these regions have relatively weak public health systems and so cannot support large-scale vector counting efforts. In this paper, we present a system architecture and a prototype to count mosquitoes in the wild with an Internet of Things approach. A sensor board is developed to gather audio data, and models are developed to detect, classify, and count mosquito species. Here, we present our prototype and an extensive background study of classifying mosquitoes based on sound recordings and some preliminary results and discussion.},
  isbn = {978-1-4503-8478-0},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/GJM73PPH/Vasconcelos et al. - 2021 - Counting Mosquitoes in the Wild An Internet of Th.pdf}
}

@inproceedings{wangPuffEMEcigaretteSleeve2025,
  title = {{{PuffEM}}: {{An E-cigarette Sleeve}} for {{Estimating User Nicotine Intake}}},
  booktitle = {{{ACM}}/{{IEEE International Conference}} on {{Connected Health}}: {{Applications}}, {{Systems}} and {{Engineering Technologies}} ({{CHASE}} '25), {{June}} 24--26, 2025},
  author = {Wang, Yiyang and Goel, Rishabh and Hassan, Sheraz and Doscher, Taegen J and Wang, Shilin and Whalen, Lexington Allen and Gandhi, Aditya S and Sangar, Yaman S and Cabral, Alex and Xu, Xuhai and Hester, Josiah and Adams, Alexander T},
  year = {2025},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/3721201.3721393},
  file = {/Users/yiyangwang/Zotero/storage/XN28F983/Wang et al. - PuffEM An E-cigarette Sleeve for Estimating User Nicotine Intake.pdf}
}

@inproceedings{wengBridgingCoachingKnowledge2025,
  title = {Bridging {{Coaching Knowledge}} and {{AI Feedback}} to {{Enhance Motor Learning}} in {{Basketball Shooting Mechanics Through}} a {{Knowledge-Based SOP Framework}}},
  booktitle = {Proceedings of the 2025 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Weng, Jian-Jia and Ku, Calvin and Wang, Jo Chien and Cheng, Chih-Jen and Lin, Tica and Su, Yu-An and Tsai, Tsung-Hsun and Lin, You-Yi and Ku, Lun-Wei and Chu, Hung-Kuo and Hu, Min-Chun},
  year = {2025},
  month = apr,
  pages = {1--20},
  publisher = {ACM},
  address = {Yokohama Japan},
  doi = {10.1145/3706598.3713324},
  urldate = {2025-05-23},
  isbn = {979-8-4007-1394-1},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/94DCMPDK/Weng et al. - 2025 - Bridging Coaching Knowledge and AI Feedback to Enhance Motor Learning in Basketball Shooting Mechani.pdf}
}

@inproceedings{wisniewskiParentalControlVs2017,
  title = {Parental {{Control}} vs. {{Teen Self-Regulation}}: {{Is}} There a Middle Ground for Mobile Online Safety?},
  shorttitle = {Parental {{Control}} vs. {{Teen Self-Regulation}}},
  booktitle = {Proceedings of the 2017 {{ACM Conference}} on {{Computer Supported Cooperative Work}} and {{Social Computing}}},
  author = {Wisniewski, Pamela and Ghosh, Arup Kumar and Xu, Heng and Rosson, Mary Beth and Carroll, John M.},
  year = {2017},
  month = feb,
  series = {{{CSCW}} '17},
  pages = {51--69},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2998181.2998352},
  urldate = {2025-02-12},
  abstract = {We conducted a structured, qualitative feature analysis of 75 Android mobile apps designed for the purpose of promoting adolescent online safety. Through this analysis we identified 42 unique features that mapped to a theoretically derived conceptual framework of teen online safety strategies balanced between parental control strategies (through monitoring, restriction, and active mediation) and teen self-regulation strategies (through self-monitoring, impulse control, and risk-coping). We found that the apps strongly favored features that promote parental control through monitoring and restricting teens' online behaviors over teen self-regulation or more communicative and collaborative practices between parents and teens. We use the lens of value sensitive design to discuss the implications of our results and identify opportunities for designing mobile apps for online safety that embed more positive family values.},
  isbn = {978-1-4503-4335-0},
  file = {/Users/yiyangwang/Zotero/storage/4VPREGJG/Wisniewski et al. - 2017 - Parental Control vs. Teen Self-Regulation Is ther.pdf}
}

@misc{xiaLanguageMultimodalModels2024,
  title = {Language and {{Multimodal Models}} in {{Sports}}: {{A Survey}} of {{Datasets}} and {{Applications}}},
  shorttitle = {Language and {{Multimodal Models}} in {{Sports}}},
  author = {Xia, Haotian and Yang, Zhengbang and Zhao, Yun and Wang, Yuqing and Li, Jingxi and Tracy, Rhys and Zhu, Zhuangdi and Wang, Yuan-fang and Chen, Hanjie and Shen, Weining},
  year = {2024},
  month = jun,
  number = {arXiv:2406.12252},
  eprint = {2406.12252},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.12252},
  urldate = {2025-05-26},
  abstract = {Recent integration of Natural Language Processing (NLP) and multimodal models has advanced the field of sports analytics. This survey presents a comprehensive review of the datasets and applications driving these innovations post-2020. We overviewed and categorized datasets into three primary types: language-based, multimodal, and convertible datasets. Language-based and multimodal datasets are for tasks involving text or multimodality (e.g., text, video, audio), respectively. Convertible datasets, initially single-modal (video), can be enriched with additional annotations, such as explanations of actions and video descriptions, to become multimodal, offering future potential for richer and more diverse applications. Our study highlights the contributions of these datasets to various applications, from improving fan experiences to supporting tactical analysis and medical diagnostics. We also discuss the challenges and future directions in dataset development, emphasizing the need for diverse, high-quality data to support real-time processing and personalized user experiences. This survey provides a foundational resource for researchers and practitioners aiming to leverage NLP and multimodal models in sports, offering insights into current trends and future opportunities in the field.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {,Computer Science - Computation and Language},
  file = {/Users/yiyangwang/Zotero/storage/4W94V576/Xia et al. - 2024 - Language and Multimodal Models in Sports A Survey of Datasets and Applications.pdf;/Users/yiyangwang/Zotero/storage/6YR8T4P6/2406.html}
}

@article{xuCanLargeLanguage2024,
  title = {Can {{Large Language Models Be Good Companions}}?: {{An LLM-Based Eyewear System}} with {{Conversational Common Ground}}},
  shorttitle = {Can {{Large Language Models Be Good Companions}}?},
  author = {Xu, Zhenyu and Xu, Hailin and Lu, Zhouyang and Zhao, Yingying and Zhu, Rui and Wang, Yujiang and Dong, Mingzhi and Chang, Yuhu and Lv, Qin and Dick, Robert P. and Yang, Fan and Lu, Tun and Gu, Ning and Shang, Li},
  year = {2024},
  month = may,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {8},
  number = {2},
  pages = {1--41},
  issn = {2474-9567},
  doi = {10.1145/3659600},
  urldate = {2025-05-14},
  abstract = {Developing chatbots as personal companions has long been a goal of artificial intelligence researchers. Recent advances in Large Language Models (LLMs) have delivered a practical solution for endowing chatbots with anthropomorphic language capabilities. However, it takes more than LLMs to enable chatbots that can act as companions. Humans use their understanding of individual personalities to drive conversations. Chatbots also require this capability to enable human-like companionship. They should act based on personalized, real-time, and time-evolving knowledge of their users. We define such essential knowledge as the common ground between chatbots and their users, and we propose to build a common-ground-aware dialogue system from an LLM-based module, named OS-1, to enable chatbot companionship. Hosted by eyewear, OS-1 can sense the visual and audio signals the user receives and extract real-time contextual semantics. Those semantics are categorized and recorded to formulate historical contexts from which the user's profile is distilled and evolves over time, i.e., OS-1 gradually learns about its user. OS-1 combines knowledge from real-time semantics, historical contexts, and user-specific profiles to produce a common-ground-aware prompt input into the LLM module. The LLM's output is converted to audio, spoken to the wearer when appropriate. We conduct laboratory and in-field studies to assess OS-1's ability to build common ground between the chatbot and its user. The technical feasibility and capabilities of the system are also evaluated. Our results show that by utilizing personal context, OS-1 progressively develops a better understanding of its users. This enhances user satisfaction and potentially leads to various personal service scenarios, such as emotional support and assistance.},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/XUGKX2ZW/Xu et al. - 2024 - Can Large Language Models Be Good Companions An LLM-Based Eyewear System with Conversational Commo.pdf}
}

@misc{yangSetofMarkPromptingUnleashes2023,
  title = {Set-of-{{Mark Prompting Unleashes Extraordinary Visual Grounding}} in {{GPT-4V}}},
  author = {Yang, Jianwei and Zhang, Hao and Li, Feng and Zou, Xueyan and Li, Chunyuan and Gao, Jianfeng},
  year = {2023},
  month = nov,
  number = {arXiv:2310.11441},
  eprint = {2310.11441},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.11441},
  urldate = {2025-06-06},
  abstract = {We present Set-of-Mark (SoM), a new visual prompting method, to unleash the visual grounding abilities of large multimodal models (LMMs), such as GPT-4V. As illustrated in Fig. 1 (right), we employ off-the-shelf interactive segmentation models, such as SEEM/SAM, to partition an image into regions at different levels of granularity, and overlay these regions with a set of marks e.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can answer the questions that require visual grounding. We perform a comprehensive empirical study to validate the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks. For example, our experiments show that GPT-4V with SoM in zero-shot setting outperforms the state-of-the-art fully-finetuned referring expression comprehension and segmentation model on RefCOCOg. Code for SoM prompting is made public at: https://github.com/microsoft/SoM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction},
  file = {/Users/yiyangwang/Zotero/storage/HFQBHUBU/Yang et al. - 2023 - Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V.pdf;/Users/yiyangwang/Zotero/storage/H33XV57W/2310.html}
}

@misc{yangSetofMarkPromptingUnleashes2023a,
  title = {Set-of-{{Mark Prompting Unleashes Extraordinary Visual Grounding}} in {{GPT-4V}}},
  author = {Yang, Jianwei and Zhang, Hao and Li, Feng and Zou, Xueyan and Li, Chunyuan and Gao, Jianfeng},
  year = {2023},
  month = nov,
  number = {arXiv:2310.11441},
  eprint = {2310.11441},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.11441},
  urldate = {2025-06-06},
  abstract = {We present Set-of-Mark (SoM), a new visual prompting method, to unleash the visual grounding abilities of large multimodal models (LMMs), such as GPT-4V. As illustrated in Fig. 1 (right), we employ off-the-shelf interactive segmentation models, such as SEEM/SAM, to partition an image into regions at different levels of granularity, and overlay these regions with a set of marks e.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can answer the questions that require visual grounding. We perform a comprehensive empirical study to validate the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks. For example, our experiments show that GPT-4V with SoM in zero-shot setting outperforms the state-of-the-art fully-finetuned referring expression comprehension and segmentation model on RefCOCOg. Code for SoM prompting is made public at: https://github.com/microsoft/SoM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction},
  file = {/Users/yiyangwang/Zotero/storage/NL39ASQS/Yang et al. - 2023 - Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V.pdf;/Users/yiyangwang/Zotero/storage/92XUWI5H/2310.html}
}

@article{zhangChatMatchExploringPotential2025,
  title = {{{ChatMatch}}: {{Exploring}} the Potential of Hybrid Vision--Language Deep Learning Approach for the Intelligent Analysis and Inference of Racket Sports},
  shorttitle = {{{ChatMatch}}},
  author = {Zhang, Jiawen and Han, Dongliang and Han, Shuai and Li, Heng and Lam, Wing-Kai and Zhang, Mingyu},
  year = {2025},
  month = jan,
  journal = {Computer Speech \& Language},
  volume = {89},
  pages = {101694},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2024.101694},
  urldate = {2025-05-26},
  abstract = {Video understanding technology has become increasingly important in various disciplines, yet current approaches have primarily focused on lower comprehension level of video content, posing challenges for providing comprehensive and professional insights at a higher comprehension level. Video analysis plays a crucial role in athlete training and strategy development in racket sports. This study aims to demonstrate an innovative and higher-level video comprehension framework (ChatMatch), which integrates computer vision technologies with the cutting-edge large language models (LLM) to enable intelligent analysis and inference of racket sports videos. To examine the feasibility of this framework, we deployed a prototype of ChatMatch in the badminton in this study. A vision-based encoder was first proposed to extract the meta-features included the locations, actions, gestures, and action results of players in each frame of racket match videos, followed by a rule-based decoding method to transform the extracted information in both structured knowledge and unstructured knowledge. A set of LLM-based agents included namely task identifier, coach agent, statistician agent, and video manager, was developed through a prompt engineering and driven by an automated mechanism. The automatic collaborative interaction among the agents enabled the provision of a comprehensive response to professional inquiries from users. The validation findings showed that our vision models had excellent performances in meta-feature extraction, achieving a location identification accuracy of 0.991, an action recognition accuracy of 0.902, and a gesture recognition accuracy of 0.950. Additionally, a total of 100 questions were gathered from four proficient badminton players and one coach to evaluate the performance of the LLM-based agents, and the outcomes obtained from ChatMatch exhibited commendable results across general inquiries, statistical queries, and video retrieval tasks. These findings highlight the potential of using this approach that can offer valuable insights for athletes and coaches while significantly improve the efficiency of sports video analysis.},
  langid = {american},
  keywords = {,Badminton,Deep learning,Expert system,Large language model,Video understanding},
  file = {/Users/yiyangwang/Zotero/storage/W2NB2ACU/S0885230824000779.html}
}

@article{zhangFlexibleComputationalPhotodetectors2022,
  title = {Flexible Computational Photodetectors for Self-Powered Activity Sensing},
  author = {Zhang, Dingtian and {Fuentes-Hernandez}, Canek and Vijayan, Raaghesh and Zhang, Yang and Li, Yunzhi and Park, Jung Wook and Wang, Yiyang and Zhao, Yuhui and Arora, Nivedita and Mirzazadeh, Ali and Do, Youngwook and Cheng, Tingyu and Swaminathan, Saiganesh and Starner, Thad and Andrew, Trisha L. and Abowd, Gregory D.},
  year = {2022},
  month = jan,
  journal = {npj Flexible Electronics},
  volume = {6},
  number = {1},
  pages = {1--8},
  publisher = {Nature Publishing Group},
  issn = {2397-4621},
  doi = {10.1038/s41528-022-00137-z},
  urldate = {2024-12-30},
  abstract = {Conventional vision-based systems, such as cameras, have demonstrated their enormous versatility in sensing human activities and developing interactive environments. However, these systems have long been criticized for incurring privacy, power, and latency issues due to their underlying structure of pixel-wise analog signal acquisition, computation, and communication. In this research, we overcome these limitations by introducing in-sensor analog computation through the distribution of interconnected photodetectors in space, having a weighted responsivity, to create what we call a computational photodetector. Computational photodetectors can be used to extract mid-level vision features as a single continuous analog signal measured via a two-pin connection. We develop computational photodetectors using thin and flexible low-noise organic photodiode arrays coupled with a self-powered wireless system to demonstrate a set of designs that capture position, orientation, direction, speed, and identification information, in a range of applications from explicit interactions on everyday surfaces to implicit activity detection.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Electrical and electronic engineering,Sensors},
  file = {/Users/yiyangwang/Zotero/storage/S6UZUEWH/Zhang et al. - 2022 - Flexible computational photodetectors for self-pow.pdf}
}

@article{zhangFlexibleComputationalPhotodetectors2022a,
  title = {Flexible Computational Photodetectors for Self-Powered Activity Sensing},
  author = {Zhang, Dingtian and {Fuentes-Hernandez}, Canek and Vijayan, Raaghesh and Zhang, Yang and Li, Yunzhi and Park, Jung Wook and Wang, Yiyang and Zhao, Yuhui and Arora, Nivedita and Mirzazadeh, Ali and Do, Youngwook and Cheng, Tingyu and Swaminathan, Saiganesh and Starner, Thad and Andrew, Trisha L. and Abowd, Gregory D.},
  year = {2022},
  month = jan,
  journal = {npj Flexible Electronics},
  volume = {6},
  number = {1},
  pages = {1--8},
  issn = {2397-4621},
  doi = {10.1038/s41528-022-00137-z},
  urldate = {2024-12-30},
  abstract = {Conventional vision-based systems, such as cameras, have demonstrated their enormous versatility in sensing human activities and developing interactive environments. However, these systems have long been criticized for incurring privacy, power, and latency issues due to their underlying structure of pixel-wise analog signal acquisition, computation, and communication. In this research, we overcome these limitations by introducing in-sensor analog computation through the distribution of interconnected photodetectors in space, having a weighted responsivity, to create what we call a computational photodetector. Computational photodetectors can be used to extract mid-level vision features as a single continuous analog signal measured via a two-pin connection. We develop computational photodetectors using thin and flexible low-noise organic photodiode arrays coupled with a self-powered wireless system to demonstrate a set of designs that capture position, orientation, direction, speed, and identification information, in a range of applications from explicit interactions on everyday surfaces to implicit activity detection.},
  keywords = {Electrical and electronic engineering,Sensors},
  file = {/Users/yiyangwang/Zotero/storage/53993UPS/Zhang et al. - 2022 - Flexible computational photodetectors for self-pow.pdf}
}

@article{zhangNeckSenseMultiSensorNecklace2020,
  title = {{{NeckSense}}: {{A Multi-Sensor Necklace}} for {{Detecting Eating Activities}} in {{Free-Living Conditions}}},
  shorttitle = {{{NeckSense}}},
  author = {Zhang, Shibo and Zhao, Yuqi and Nguyen, Dzung Tri and Xu, Runsheng and Sen, Sougata and Hester, Josiah and Alshurafa, Nabil},
  year = {2020},
  month = jun,
  journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  volume = {4},
  number = {2},
  pages = {72:1--72:26},
  doi = {10.1145/3397313},
  urldate = {2024-11-12},
  abstract = {We present the design, implementation, and evaluation of a multi-sensor, low-power necklace, NeckSense, for automatically and unobtrusively capturing fine-grained information about an individual's eating activity and eating episodes, across an entire waking day in a naturalistic setting. NeckSense fuses and classifies the proximity of the necklace from the chin, the ambient light, the Lean Forward Angle, and the energy signals to determine chewing sequences, a building block of the eating activity. It then clusters the identified chewing sequences to determine eating episodes. We tested NeckSense on 11 participants with and 9 participants without obesity, across two studies, where we collected more than 470 hours of data in a naturalistic setting. Our results demonstrate that NeckSense enables reliable eating detection for individuals with diverse body mass index (BMI) profiles, across an entire waking day, even in free-living environments. Overall, our system achieves an F1-score of 81.6\% in detecting eating episodes in an exploratory study. Moreover, our system can achieve an F1-score of 77.1\% for episodes even in an all-day-long free-living setting. With more than 15.8 hours of battery life, NeckSense will allow researchers and dietitians to better understand natural chewing and eating behaviors. In the future, researchers and dietitians can use NeckSense to provide appropriate real-time interventions when an eating episode is detected or when problematic eating is identified.},
  file = {/Users/yiyangwang/Zotero/storage/976DRC5Z/Zhang et al. - 2020 - NeckSense A Multi-Sensor Necklace for Detecting E.pdf}
}

@article{zhangOptoSenseUbiquitousSelfPowered2020,
  title = {{{OptoSense}}: {{Towards Ubiquitous Self-Powered Ambient Light Sensing Surfaces}}},
  shorttitle = {{{OptoSense}}},
  author = {Zhang, Dingtian and Park, Jung Wook and Zhang, Yang and Zhao, Yuhui and Wang, Yiyang and Li, Yunzhi and Bhagwat, Tanvi and Chou, Wen-Fang and Jia, Xiaojia and Kippelen, Bernard and {Fuentes-Hernandez}, Canek and Starner, Thad and Abowd, Gregory D.},
  year = {2020},
  month = sep,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {4},
  number = {3},
  pages = {1--27},
  issn = {2474-9567},
  doi = {10.1145/3411826},
  urldate = {2024-12-30},
  abstract = {Ubiquitous computing requires robust and sustainable sensing techniques to detect users for explicit and implicit inputs. Existing solutions with cameras can be privacy-invasive. Battery-powered sensors require user maintenance, preventing practical ubiquitous sensor deployment. We present OptoSense, a general-purpose self-powered sensing system which senses ambient light at the surface level of everyday objects as a high-fidelity signal to infer user activities and interactions. To situate the novelty of OptoSense among prior work and highlight the generalizability of the approach, we propose a design framework of ambient light sensing surfaces, enabling implicit activity sensing and explicit interactions in a wide range of use cases with varying sensing dimensions (0D, 1D, 2D), fields of view (wide, narrow), and perspectives (egocentric, allocentric). OptoSense supports this framework through example applications ranging from object use and indoor traffic detection, to liquid sensing and multitouch input. Additionally, the system can achieve high detection accuracy while being self-powered by ambient light. On-going improvements that replace Optosense's silicon-based sensors with organic semiconductors (OSCs) enable devices that are ultra-thin, flexible, and cost effective to scale.},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/F85BIYJX/Zhang et al. - 2020 - OptoSense Towards Ubiquitous Self-Powered Ambient.pdf}
}

@article{zhangOptoSenseUbiquitousSelfPowered2020a,
  title = {{{OptoSense}}: {{Towards Ubiquitous Self-Powered Ambient Light Sensing Surfaces}}},
  shorttitle = {{{OptoSense}}},
  author = {Zhang, Dingtian and Park, Jung Wook and Zhang, Yang and Zhao, Yuhui and Wang, Yiyang and Li, Yunzhi and Bhagwat, Tanvi and Chou, Wen-Fang and Jia, Xiaojia and Kippelen, Bernard and {Fuentes-Hernandez}, Canek and Starner, Thad and Abowd, Gregory D.},
  year = {2020},
  month = sep,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {4},
  number = {3},
  pages = {1--27},
  issn = {2474-9567},
  doi = {10.1145/3411826},
  urldate = {2024-12-30},
  abstract = {Ubiquitous computing requires robust and sustainable sensing techniques to detect users for explicit and implicit inputs. Existing solutions with cameras can be privacy-invasive. Battery-powered sensors require user maintenance, preventing practical ubiquitous sensor deployment. We present OptoSense, a general-purpose self-powered sensing system which senses ambient light at the surface level of everyday objects as a high-fidelity signal to infer user activities and interactions. To situate the novelty of OptoSense among prior work and highlight the generalizability of the approach, we propose a design framework of ambient light sensing surfaces, enabling implicit activity sensing and explicit interactions in a wide range of use cases with varying sensing dimensions (0D, 1D, 2D), fields of view (wide, narrow), and perspectives (egocentric, allocentric). OptoSense supports this framework through example applications ranging from object use and indoor traffic detection, to liquid sensing and multitouch input. Additionally, the system can achieve high detection accuracy while being self-powered by ambient light. On-going improvements that replace Optosense's silicon-based sensors with organic semiconductors (OSCs) enable devices that are ultra-thin, flexible, and cost effective to scale.},
  file = {/Users/yiyangwang/Zotero/storage/4KY62QQ4/Zhang et al. - 2020 - OptoSense Towards Ubiquitous Self-Powered Ambient.pdf}
}

@inproceedings{zhangPromptingEmbodiedAI2025,
  title = {Prompting an {{Embodied AI Agent}}: {{How Embodiment}} and {{Multimodal Signaling Affects Prompting Behaviour}}},
  shorttitle = {Prompting an {{Embodied AI Agent}}},
  booktitle = {Proceedings of the 2025 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Zhang, Tianyi and Au Yeung, Colin and Aurelia, Emily and Onishi, Yuki and Chulpongsatorn, Neil and Li, Jiannan and Tang, Anthony},
  year = {2025},
  month = apr,
  series = {{{CHI}} '25},
  pages = {1--25},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3706598.3713110},
  urldate = {2025-07-08},
  abstract = {Current voice agents wait for a user to complete their verbal instruction before responding; yet, this is misaligned with how humans engage in everyday conversational interaction, where interlocutors use multimodal signaling (e.g. nodding, grunting, or looking at referred to objects) to ensure conversational grounding. We designed an embodied VR agent that exhibits multimodal signaling behaviors in response to situated prompts, by turning its head, or by visually highlighting objects being discussed or referred to. We explore how people prompt this agent to design and manipulate the objects in a VR scene. Through a Wizard of Oz study, we found that participants interacting with an agent that indicated its understanding of spatial and action references were able to prevent errors 30\% of the time, and were more satisfied and confident in the agent's abilities. These findings underscore the importance of designing multimodal signaling communication techniques for future embodied agents.},
  isbn = {979-8-4007-1394-1},
  langid = {american},
  file = {/Users/yiyangwang/Zotero/storage/8MPQPFGH/Zhang et al. - 2025 - Prompting an Embodied AI Agent How Embodiment and Multimodal Signaling Affects Prompting Behaviour.pdf}
}

@inproceedings{zhangShortRealTimeBladder2023,
  title = {Short: {{Real-Time Bladder Monitoring}} by {{Bio-impedance Analysis}} to {{Aid Urinary Incontinence}}},
  shorttitle = {Short},
  booktitle = {Proceedings of the 8th {{ACM}}/{{IEEE International Conference}} on {{Connected Health}}: {{Applications}}, {{Systems}} and {{Engineering Technologies}}},
  author = {Zhang, Ruoyu and Fang, Ruijie and Zhang, Zhichao and Hosseini, Elahe and Orooji, Mahdi and Homayoun, Houman and {Goncu-Berk}, Gozde},
  year = {2023},
  month = jun,
  pages = {138--142},
  publisher = {ACM},
  address = {Orlando FL USA},
  doi = {10.1145/3580252.3586985},
  urldate = {2024-11-22},
  isbn = {979-8-4007-0102-3},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/IW9B2AVC/Zhang et al. - 2023 - Short Real-Time Bladder Monitoring by Bio-impedan.pdf}
}

@inproceedings{zhiGameViewsUnderstandingSupporting2019,
  title = {{{GameViews}}: {{Understanding}} and {{Supporting Data-driven Sports Storytelling}}},
  shorttitle = {{{GameViews}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Zhi, Qiyu and Lin, Suwen and Talkad Sukumar, Poorna and Metoyer, Ronald},
  year = {2019},
  month = may,
  pages = {1--13},
  publisher = {ACM},
  address = {Glasgow Scotland Uk},
  doi = {10.1145/3290605.3300499},
  urldate = {2025-05-13},
  isbn = {978-1-4503-5970-2},
  langid = {english},
  file = {/Users/yiyangwang/Zotero/storage/32N98CRG/Zhi et al. - 2019 - GameViews Understanding and Supporting Data-driven Sports Storytelling.pdf}
}

@inproceedings{zhu-tianIBallAugmentingBasketball2023,
  title = {{{iBall}}: {{Augmenting Basketball Videos}} with {{Gaze-moderated Embedded Visualizations}}},
  shorttitle = {{{iBall}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {{Zhu-Tian}, Chen and Yang, Qisen and Shan, Jiarui and Lin, Tica and Beyer, Johanna and Xia, Haijun and Pfister, Hanspeter},
  year = {2023},
  month = apr,
  eprint = {2303.03476},
  primaryclass = {cs},
  pages = {1--18},
  doi = {10.1145/3544548.3581266},
  urldate = {2025-05-12},
  abstract = {We present iBall, a basketball video-watching system that leverages gaze-moderated embedded visualizations to facilitate game understanding and engagement of casual fans. Video broadcasting and online video platforms make watching basketball games increasingly accessible. Yet, for new or casual fans, watching basketball videos is often confusing due to their limited basketball knowledge and the lack of accessible, on-demand information to resolve their confusion. To assist casual fans in watching basketball videos, we compared the game-watching behaviors of casual and die-hard fans in a formative study and developed iBall based on the fndings. iBall embeds visualizations into basketball videos using a computer vision pipeline, and automatically adapts the visualizations based on the game context and users' gaze, helping casual fans appreciate basketball games without being overwhelmed. We confrmed the usefulness, usability, and engagement of iBall in a study with 16 casual fans, and further collected feedback from 8 die-hard fans.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {,Computer Science - Graphics,Computer Science - Human-Computer Interaction},
  file = {/Users/yiyangwang/Zotero/storage/7RBMYVLI/Zhu-Tian et al. - 2023 - iBall Augmenting Basketball Videos with Gaze-moderated Embedded Visualizations.pdf;/Users/yiyangwang/Zotero/storage/32NWKKAB/2303.html}
}

@misc{zhuMultiAgentBenchEvaluatingCollaboration2025,
  title = {{{MultiAgentBench}}: {{Evaluating}} the {{Collaboration}} and {{Competition}} of {{LLM}} Agents},
  shorttitle = {{{MultiAgentBench}}},
  author = {Zhu, Kunlun and Du, Hongyi and Hong, Zhaochen and Yang, Xiaocheng and Guo, Shuyi and Wang, Zhe and Wang, Zhenhailong and Qian, Cheng and Tang, Xiangru and Ji, Heng and You, Jiaxuan},
  year = {2025},
  month = mar,
  number = {arXiv:2503.01935},
  eprint = {2503.01935},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.01935},
  urldate = {2025-06-19},
  abstract = {Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3\%. Code and datasets are public available at https://github.com/MultiagentBench/MARBLE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Multiagent Systems},
  file = {/Users/yiyangwang/Zotero/storage/4AAVPJHL/Zhu et al. - 2025 - MultiAgentBench Evaluating the Collaboration and Competition of LLM agents.pdf;/Users/yiyangwang/Zotero/storage/BHMFLRZ4/2503.html}
}

@misc{zhuMultiAgentBenchEvaluatingCollaboration2025a,
  title = {{{MultiAgentBench}}: {{Evaluating}} the {{Collaboration}} and {{Competition}} of {{LLM}} Agents},
  shorttitle = {{{MultiAgentBench}}},
  author = {Zhu, Kunlun and Du, Hongyi and Hong, Zhaochen and Yang, Xiaocheng and Guo, Shuyi and Wang, Zhe and Wang, Zhenhailong and Qian, Cheng and Tang, Xiangru and Ji, Heng and You, Jiaxuan},
  year = {2025},
  month = mar,
  number = {arXiv:2503.01935},
  eprint = {2503.01935},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.01935},
  urldate = {2025-06-19},
  abstract = {Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3\%. Code and datasets are public available at https://github.com/MultiagentBench/MARBLE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Multiagent Systems},
  file = {/Users/yiyangwang/Zotero/storage/ZC7WD34G/Zhu et al. - 2025 - MultiAgentBench Evaluating the Collaboration and Competition of LLM agents.pdf;/Users/yiyangwang/Zotero/storage/6UXYUMIU/2503.html}
}
